{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Group Number:</b> 7\n",
    "<br><b>Name Group Member 1:</b>   Paraa Afifi\n",
    "<br><b>u-Kürzel Group Member 1:</b>   uppns\n",
    "<br><b>Name Group Member 2:</b>   Dan-Jason Bräuninger\n",
    "<br><b>u-Kürzel Group Member 2:</b>   uuuab\n",
    "<br><b>Name Group Member 3:</b> Sami Shahzad\n",
    "<br><b>u-Kürzel Group Member 3:</b>uvoei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Classification with Neural Networks\n",
    "In this chapter, you will understand the workings of a classifier and manually train one that operates on a single value. You will improve the classifier step by step and learn fundamental concepts about classification as you go along.\n",
    "Finally, you will use automated backpropagation to train a multi-layer neural network to emulate a logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction\n",
    "In machine learning and statistics, classification is the problem of identifying to which set of categories (sub-populations) a new observation belongs to, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class or assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). [1]\n",
    "\n",
    "A classification process requires a dataset that is split into different categories. A classifier can be trained on this dataset by learning the relationship between certain properties of the input data and the corresponding categories. \n",
    "To classify new data, the process is similar as in the chapter \"Regression\", however additional computational steps can be added depending on the application.\n",
    "A common classification problem that can be solved by neural networks is image recognition (seen in Figure 1).\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/neural_network_classification.png\" alt=\" Image recognition by a neural net\" width=\"800\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 1 - Image recognition by a neural network</em></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # Used to allow referencing classes that have not yet been defined in type annotations. This will become default behaviour in Python 3.10. Until then, we have to use this line to enable that behaviour\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interact, Layout, FloatSlider\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "import threading\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input_val: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(input_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(predictions: np.ndarray, solutions: np.ndarray) -> float:\n",
    "    total_squared_loss = np.sum(np.subtract(predictions, solutions)**2) #np allows to handle both values and lists\n",
    "    mean_squared_loss = total_squared_loss/len(predictions)\n",
    "    return mean_squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight: float, bias: float):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self) -> float:\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self) -> float:\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.activation = np.dot(self.weight, x) + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points_red: Dict[str, List[float]], points_blue: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], loss_function: Callable[[np.ndarray, np.ndarray], float] = mean_squared_loss, loss_string: str = \"Loss\", width: int = 800, height: int = 400, margin: Dict[str, int] = { 't': 0, 'l': 170 }, draw_time: float = 0.1):\n",
    "        self.idle = True\n",
    "        self.points_red = points_red\n",
    "        self.points_blue = points_blue\n",
    "        self.draw_time = draw_time\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_string = loss_string\n",
    "\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.01)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.01)\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Neck height in m\", range=ranges[\"x\"]),\n",
    "            yaxis=dict(title=\"y\", range=ranges[\"y\"]),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "\n",
    "        self.plot_points_red = go.Scatter(\n",
    "            x=points_red[\"x\"], y=points_red[\"y\"], mode=\"markers\", marker=dict(color='rgb(255, 0, 0)', size=10)\n",
    "        )\n",
    "        self.plot_points_blue = go.Scatter(\n",
    "            x=points_blue[\"x\"],\n",
    "            y=points_blue[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color='rgb(0, 0, 255)', size=10, symbol=\"square\"),\n",
    "        )\n",
    "\n",
    "        self.plot_point_new = go.Scatter(\n",
    "            x=[], y=[], mode=\"markers\", marker=dict(size=20, symbol=\"star\", color='rgb(0,0,0)')\n",
    "        )\n",
    "\n",
    "        self.data = [self.trace, self.plot_points_red, self.plot_points_blue, self.plot_point_new]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        loss_red = self.loss_function(self.neuron.compute(self.points_red[\"x\"]), self.points_red[\"y\"])\n",
    "        loss_blue = self.loss_function(self.neuron.compute(self.points_blue[\"x\"]), self.points_blue[\"y\"])\n",
    "        print(self.loss_string,\": {:0.3f}\".format((loss_red + loss_blue) / 2))\n",
    "\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 From Regression to Classification\n",
    "\n",
    "###  2.2.1 Linear Regression\n",
    "\n",
    "You find yourself working on a farm with sheep and llamas grazing in separate enclosures. However, last night the shepard forgot to close the gate between the two enclosures. The llamas and sheep now are mixed and have to be separated again. You immediately come up with a machine learning based solution to separate the sheep from the llamas again: You assume that llamas can be distinguished from sheep by measuring the distance from the top of their head to their spine, since llamas have significantly longer necks. Using a LIDAR scanner, neck heights will be measured autonomously and the animals will be separated using a food enticement and an electronic turnstile that only lets llamas through.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/neck_heights.png\" alt=\"Concept of Neck Height Measurement\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 2: Concept of Neck Height Measurement.</em></p>\n",
    "\n",
    "\n",
    "To collect sample data, you go out on the field with a measuring tape and measure the neck heights of some sheep and llamas. You specify two categories: '0' for sheep and '1' for llamas. (See table 1)\n",
    "\n",
    "Most llamas are grown up and have long necks, but there are also some young llamas with smaller necks. However, since their necks are still longer than the sheeps', you figure that this won't be a problem.\n",
    "\n",
    "|  Animal | Neck height  | Category  |\n",
    "|---------|--------------|-----------|\n",
    "| Sheep #1| 0.20m        |0          |\n",
    "| Sheep #2| 0.23m        |0          |\n",
    "| Sheep #3| 0.28m        |0          |\n",
    "| Sheep #4| 0.32m        |0          |\n",
    "| Sheep #5| 0.35m        |0          |\n",
    "| Llama #1| 0.55m        |1          |\n",
    "| Llama #2| 0.68m        |1          |\n",
    "| Llama #3| 0.74m        |1          |\n",
    "| Llama #4| 0.83m        |1          |\n",
    "| Llama #5| 0.95m        |1          |\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 1 - Your data mining results\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.1 Training a Linear Regression Neuron by Hand\n",
    "For the sake of simplicity, you start by using a single neuron as a classifier. Run the two cells below to define the data mining points and to display a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_sheep = dict(\n",
    "              x=[ 0.20, 0.23, 0.28, 0.32, 0.35],\n",
    "              y=[ 0, 0, 0, 0, 0]\n",
    "             )\n",
    "\n",
    "points_llamas = dict(\n",
    "              x=[ 0.55, 0.68, 0.74, 0.83, 0.95],\n",
    "              y=[ 1,  1, 1, 1, 1]\n",
    "             )\n",
    "\n",
    "ranges = dict(x=[-0.1, 1.25], y=[-0.5, 1.4])\n",
    "slider_layout = Layout(width=\"90%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b393325c9f214644b321c8e297a0cf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=4.0, min=-2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024c95f0555d42c3be49e3411863f7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '0aafdaad-bb28-4220-8102-d83fd7c72f59',\n",
       "              'x': {'bdata': ('mpmZmZmZub8L16NwPQq3v3wUrkfher' ... 'tRuB6F8z+rR+F6FK7zP9SjcD0K1/M/'),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAA4L9cj8L1KFzfv7gehetRuN' ... 'gehev1PxyuR+F6FPY/RQrXo3A99j8='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'marker': {'color': 'rgb(255, 0, 0)', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '22eb4423-90b2-4b4a-8a55-ed6a4cdae0a8',\n",
       "              'x': [0.2, 0.23, 0.28, 0.32, 0.35],\n",
       "              'y': [0, 0, 0, 0, 0]},\n",
       "             {'marker': {'color': 'rgb(0, 0, 255)', 'size': 10, 'symbol': 'square'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '035bf352-58e5-4429-a96d-0efe7881820a',\n",
       "              'x': [0.55, 0.68, 0.74, 0.83, 0.95],\n",
       "              'y': [1, 1, 1, 1, 1]},\n",
       "             {'marker': {'color': 'rgb(0,0,0)', 'size': 20, 'symbol': 'star'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '2278825d-5fb5-40e6-b9ba-166a1f7daf75',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'range': [-0.1, 1.25], 'title': {'text': 'Neck height in m'}},\n",
       "               'yaxis': {'range': [-0.5, 1.4], 'title': {'text': 'y'}}}\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1 = Interactive2DPlot(points_sheep, points_llamas, ranges, loss_string=\"Mean Squared Loss\")\n",
    "neuron1 = SimpleNeuron(plot1)\n",
    "\n",
    "interact(\n",
    "    neuron1.set_values,\n",
    "    weight=FloatSlider(min=-2, max=4, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-1, max=1, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "plot1.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> Change the weight and bias sliders above. What is a weight and bias combination that results in a loss < 0.05?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer: With weight 1.6 and bias -0.3 we became a LOSS of 0.042 </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2.2.1.2 Working our way towards a discrete classifier\n",
    "Now we want to use our trained neuron to classify new neck heights. To do that, we have to write a program that takes in a neck height and outputs what the trained neuron thinks about it. The classifier will also plot the new neck height. Run the box below to get the values from the task before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss : 0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78417351cda7452e9cbc8584432fede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '27e40f27-8d76-45b2-ad14-3b8c251e28a6',\n",
       "              'x': {'bdata': ('mpmZmZmZub8L16NwPQq3v3wUrkfher' ... 'tRuB6F8z+rR+F6FK7zP9SjcD0K1/M/'),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAA4L9cj8L1KFzfv7gehetRuN' ... 'gehev1PxyuR+F6FPY/RQrXo3A99j8='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'marker': {'color': 'rgb(255, 0, 0)', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '29ce6722-9dcf-431c-8726-334b4d020536',\n",
       "              'x': [0.2, 0.23, 0.28, 0.32, 0.35],\n",
       "              'y': [0, 0, 0, 0, 0]},\n",
       "             {'marker': {'color': 'rgb(0, 0, 255)', 'size': 10, 'symbol': 'square'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e039267a-6a98-4e07-a4a8-a83974e4e83d',\n",
       "              'x': [0.55, 0.68, 0.74, 0.83, 0.95],\n",
       "              'y': [1, 1, 1, 1, 1]},\n",
       "             {'marker': {'color': 'rgb(0,0,0)', 'size': 20, 'symbol': 'star'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '99e3ea34-4243-4000-bced-398e3263c1d7',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'range': [-0.1, 1.25], 'title': {'text': 'Neck height in m'}},\n",
       "               'yaxis': {'range': [-0.5, 1.4], 'title': {'text': 'y'}}}\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a duplicate of the last plot, so you don't have to scroll\n",
    "plot2 = Interactive2DPlot(points_sheep, points_llamas, ranges, loss_string=\"Mean Squared Loss\") \n",
    "neuron2 = SimpleNeuron(plot2)\n",
    "neuron2.set_values(neuron1.get_weight(), neuron1.get_bias()) #get your values from last task\n",
    "\n",
    "plot2.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Try to implement a classifier using just a linear neuron. (Yes, an almost futile task, but this will make sense later). <br> Complete the python code below and receive a classification_result.\n",
    "<ul>\n",
    "    <li> the classification result shall be the output of neuron2, given the new neck height </li>\n",
    "    <li> you shouldn't need to add more than 1 line of code </li>\n",
    "    <li> after executing, take a look at the star in the plot above. It represents the current input/output for the new neck length</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0.0\n"
     ]
    }
   ],
   "source": [
    "new_neck_height = 0.9  # this value shall be varied to answer the questions below\n",
    "\n",
    "classification_result: float\n",
    "\n",
    "### STUDENT CODE HERE (1pt)\n",
    "classification_result = neuron2.compute(new_neck_height)\n",
    "### STUDENT CODE until HERE\n",
    "\n",
    "plot2.plot.data[3].x = [new_neck_height] #update plot\n",
    "plot2.plot.data[3].y = [classification_result] \n",
    "\n",
    "print(\"Result:\", classification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (5 pts):</b>  Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. What classification value does the smallest llama have? (run the cell above and change new_neck_height) <br>\n",
    "2. What classification value do animals with a neck height of 0.1m or 0.9m have? <br>\n",
    "3. Why is the classification value continuous, even though the training data had only two discrete values? <br>\n",
    "4. How would you interpret this continuous classification value? Try to describe it in a few words, there is no single correct answer. <br>\n",
    "5. Your neuron outputs a continuous value, but what we need is a discrete output, that clearly says either \"llama\" or \"sheep\". To do this, you add a simple decision to the output of the neuron. The decision should be approximately just as sensitive towards llamas as to sheep. What neuron output (y-value) would you choose as the threshold and why? (no single correct answer) <br>\n",
    "6. You want to add more data to your model to improve its performance. As you collect more data, you find a small llama with a neck height of 0.40m in your dataset. After you train your model on the new data, your discrete classifier decides that this small llama is a sheep. (Remember: the decision at the end only gets the y-value). Why is it problematic in this case to use a linear regression model for discrete classification? What property of the approximation function should be different? <br>\n",
    "7. You decide that that manually adding a discrete decision at the end of your network is an unpractical idea. It would be better to improve the linear neuron by adding a heaviside step function as an activation function, just like adding a ReLu function. Then the training could be automated and the right threshold could be found automatically. What is the problem with this approach if we still want to use the Backpropagation algorithm? <br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "\n",
    "1. The shortest llama has a classification value of 0.5, and exactly at this point, we have the boundary between the llama and the sheep.<br>\n",
    "2. An animal with a neck height of 0.1m has a classification value of -0.14, and an animal with a neck height of 0.9m has a classification value of 1.14.<br>\n",
    "3. Because a neuron establishes a continuous linear relationship between input and output (weight * input + bias = output), the two discrete data points serve as parameters for the linear relationship.<br>\n",
    "4. Je höher der Wert desto sicherer sind wir uns ( bzw. ist sich das NN ), dass es einen LAMA ist. Und je niedriger es, ist desto sicherer ist sich das NN, dass es ein SCHAF ist.<br>\n",
    "5. Ich hätte genau den Mittelwert zwischen 1 und 0 genommen also 0.5 -> dies haben wir aus den empirischen Versuche vermutet. <br>\n",
    "6. Das Problem ist, dass eine lineare Funktion nur eine gerade Trennlinie ziehen kann. Wenn die Daten sich überlappen oder komplexere Muster haben, kann eine gerade Linie sie nicht korrekt trennen. Wir bräuchten eine flexiblere Funktion, die nicht-lineare Grenzen ziehen kann.<br>\n",
    "7. Das Problem bei der heaviside step function ist, dass sie nicht Diffbar ist an dem Ursprung - Steigung -> Unendlich - Und da dieser Punkt sehr wichtig für die Klassifikation ist (genau der Entscheidungspunkt zwischen A und B ) können wir ohne ihn nicht weiteroptimieren. Also der Backpropagation Algo bricht hier zusammen.<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2.2 Logistic Regression\n",
    "\n",
    "In machine learning, the go-to assumption for an unknown two-class probability distribution is a logistic distribution.[2]\n",
    "Its cumulated function is the logistic function, of which the sigmoid function is the most used special case. (See Fig 3.)\n",
    "The sigmoid function enables a model to capture most natural occuring probability distributions.[3] (Further reading: see section \"Further Reading\" at the end of document)\n",
    "\n",
    "In the introduction of Task 2.1, we gave the neck lengths corresponding labels. \"0\" for sheep and \"1\" for llama.\n",
    "Here we can interpret the output of the neuron as the \"llama probability\": For example: An output of 1 means \"100%\" llama probability and an output of 0.2 means \"20%\" llama probability and so on.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/sigmoid.png\" alt=\"Sigmoid Activation Function\" width=\"500\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 3: Sigmoid Activation Function.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Complete Code and Train Neuron. Change the <code>SigmoidNeuron</code> class below to apply a sigmoid function to the final output.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidNeuron(SimpleNeuron): #inheriting from SimpleNeuron, \n",
    "                                   #all functions stay the same unless they are specified here\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        ### STUDENT CODE HERE (1 pt)\n",
    "        Ausgabe = super().compute(x)\n",
    "        self.activation = sigmoid(Ausgabe)\n",
    "        ### STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dad633d3fb45ff96206bddf6819d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=200.0, min=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fcb878b13644b78ad40cfaea979587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '4d93d3bb-8a86-4ff5-9834-aa4cfca53e1e',\n",
       "              'x': {'bdata': ('mpmZmZmZub8L16NwPQq3v3wUrkfher' ... 'tRuB6F8z+rR+F6FK7zP9SjcD0K1/M/'),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAA4L9cj8L1KFzfv7gehetRuN' ... 'gehev1PxyuR+F6FPY/RQrXo3A99j8='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'marker': {'color': 'rgb(255, 0, 0)', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'fddf07b3-8386-4126-921d-a294e3894dda',\n",
       "              'x': [0.55, 0.68, 0.74, 0.83, 0.95],\n",
       "              'y': [1, 1, 1, 1, 1]},\n",
       "             {'marker': {'color': 'rgb(0, 0, 255)', 'size': 10, 'symbol': 'square'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '4bed8b1e-0087-42b5-9368-9cf25753b0cd',\n",
       "              'x': [0.2, 0.23, 0.28, 0.32, 0.35],\n",
       "              'y': [0, 0, 0, 0, 0]},\n",
       "             {'marker': {'color': 'rgb(0,0,0)', 'size': 20, 'symbol': 'star'},\n",
       "              'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c9c5187a-2da7-42c7-87a8-b50e29c4660e',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'range': [-0.1, 1.25], 'title': {'text': 'Neck height in m'}},\n",
       "               'yaxis': {'range': [-0.5, 1.4], 'title': {'text': 'y'}}}\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_plot_sig = Interactive2DPlot(points_llamas, points_sheep, ranges, loss_string=\"Mean Squared Loss\")\n",
    "\n",
    "our_sig_neuron = SigmoidNeuron(classification_plot_sig)\n",
    "\n",
    "interact(\n",
    "    our_sig_neuron.set_values,\n",
    "    weight=FloatSlider(min=-50, max=200, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-50, max=50, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "classification_plot_sig.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. Give one example of an optimal weight and bias combination. <br>\n",
    "2. What advantage does a classifier have in general that also outputs a probability compared to a classifier that just outputs a binary yes/no value? (a few words). <br>\n",
    "3. Give one example how we can use the additional probability information to increase the accuracy of our seperation process. <br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. Für ( weight = 31.20 & bias = -14 ) haben wir LOSS = 0 !!<br>\n",
    "2. Wir haben hier die Möglichkeit zur Anpassung der Entscheidungsschwelle, so dass es unsere spezifische Aufgabe am besten erfüllt<br>\n",
    "3. Sagen wir in unserem Lama-Schaf Beispiel haben wir mehr Lamas als Schafe und bei Lamas gab es ein paar Ausreißer (kürzerer Hals, nah an Schaf ), die eigentlich durch das NN als Schaf erkannt werden. \n",
    "Hier können wir die Entscheidungsschwelle bewusst runter setzen, sodass wir genau diese Ausreißer wieder in dem richtigen Bereich reinholen können ( Z.B. statt eine klare Trennung bei 0.5, setzen wir diese bei 0.35) <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cross Entropy/Logarithmic Loss:\n",
    "The most common loss function for classification is cross entropy loss, also called logarithmic loss. (In the context of machine learning, they are equal). In the special case of two categories, the loss is called binary cross entropy. The binary cross entropy loss between the ground truth data value $y$ and the predicted value $\\hat{y}$ is calculated as follows:\n",
    "\n",
    "\\begin{align}\n",
    "−[y \\cdot log(\\hat{y}) + (1 − y) \\cdot log(1 − \\hat{y})]\n",
    "\\end{align}\n",
    "\n",
    "In this manner, the average of all data points is calculated w.r.t. this loss.\n",
    "It turns out that the derivative of a logarithmic loss using one hot encoding (explained below) is just the solution vector subtracted by the network output, which makes it very easy to work with.\n",
    "**Note:** Cross entropy loss can only be used, if the output values are between 0 and 1.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/cross_entropy.png\" alt=\"Log/Cross-Entropy loss func\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 4:  Logarithmic/Cross-Entropy Loss Function</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b> Calculate Squared and Cross Entropy Loss. Fill out the ??? in the table below (Markdown is fine to display the table). Use the cells below for calculations. \n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b>\n",
    "\n",
    "\n",
    "| Input         | Llama Probability  |      Squared Loss    | Cross Entropy Loss   |\n",
    "|---------------|--------------------|----------------------|----------------------|\n",
    "|    llama(1)   | 0.99               |  0.0001    |  0.0101       |\n",
    "|    sheep(0)   | 0.6                |  0.36       |  0.9163       |\n",
    "|    sheep(0)   | 0.95               |  0.9025     |  2.9957       |\n",
    "|    sheep(0)   | 0.999999           |  1   |  13.81555      |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions: np.ndarray, solutions: np.ndarray) -> float:\n",
    "    predictions += 1e-15 #in order to prevent log(0)\n",
    "    total_loss = np.sum(-(solutions*np.log(predictions)+(1-solutions)*np.log(1-predictions)))\n",
    "    avg_loss = total_loss/len(predictions)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared loss: 1.0000\n",
      "cross entropy loss: 13.8155\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([0.999999]) #insert here\n",
    "actual = np.array([0]) #insert here\n",
    "\n",
    "\n",
    "print(\"mean squared loss: {:0.4f}\".format(mean_squared_loss(predicted,actual)))\n",
    "print(\"cross entropy loss: {:0.4f}\".format(cross_entropy_loss(predicted,actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b>  Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. How do the goals of regression and classification generally differ?<br>\n",
    "2. Why do you think cross entropy loss is better suited for classification training algorithms?<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. Bei der Regression geht es darum, einen kontinuierlichen Wert vorherzusagen. Wobei es bei der Klassifikation hingegen geht es darum, Daten in verschiedene Kategorien oder Klassen einzuteilen <br>\n",
    "2. Der Cross-Entropy Loss hat die Eigenschaft, dass er bei falschen Vorhersagen stark betraft! Was dafür sorgt, dass das Modell schnell lernt, dass er einen Fehler gemacht hat. Dies sorgt dafür, dass wir das Modell in die richtige Richtung führen. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 One-Hot Encoding\n",
    "To do classification, categories have to be represented in a way that the classifier can process. Neural networks cannot understand categories directly and need a numeric representation.\n",
    "\n",
    "### 2.4.1 Disadvantages of Integer Encoding\n",
    "\n",
    "In the llama classifier, llamas were assigned the value $1$ and sheep the value $0$. One single output neuron would \"fire\", if a llama was found, and not fire, if a sheep was found. This type of representing categories is called **integer** or **label encoding**\n",
    "\n",
    "This works reasonably well for binary classification, but what if we want to distinguish between sheep, llamas and shepherd dogs?\n",
    "Doing this with just one output neuron would result in complications: \n",
    "- Dogs would need a label that is numerically higher or lower (for example $2$), implying an order (Dogs > Llamas) where there actually is none.\n",
    "- it would be necessary to interpret three different states out of one output neuron value\n",
    "\n",
    "Another disadvantage can be seen in the next question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> Suppose the encodings are: 0 for sheep, 1 for llamas and 2 for dogs. You classified 5 sheep and 5 dogs today. You want your classifier to output the average classification for today. What will the classifier say?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer: [(5*0) * (5*1)]/10= 1 -> Und die Klasse 1 ist Lama. Was ein völliger Unsinn ist! Wir hatten gar kein Lama und das NN hat sich für Lama entschieden</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Composition of One-Hot Encoding\n",
    "\n",
    "The solution for the shortcomings of integer encoding looks like this:\n",
    "\n",
    "| Input         | One Hot Encoding  | \n",
    "|---------------|--------------------|\n",
    "|    sheep   | [1,0,0]                |\n",
    "|    llama   | [0,1,0]               |\n",
    "|    dog     | [0,0,1]           |\n",
    "\n",
    "\n",
    "\n",
    "The length of the representation vector is always equal to the amount of categories. Only one element of the vector is 1 for each category (\"one-hot\").\n",
    "Using this encoding, we can conveniently use 3 output neurons for 3 different categories, so that the activation of each output neuron represents the classification score for that category.\n",
    "\n",
    "###  2.4.3 Limits of One-Hot Encoding\n",
    "One-hot encoding is not an unimprovable solution to represent categories, but rather another tool in the box that happens to work well for many problems, but not for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pts):</b> Suppose you would like to train a speech recognition neural network that can classify all English words contained in the Oxford English Dictionary. It does not need to classify whole sentences, just single words. What would be a problem using one-hot encoding?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:hahaha, dann bräuchten wir für jedes Wort einen Eintrag in dem OHE Vektor -> D.h. der Vektor hat dann > 500.000 Einträge :(</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Softmax Activation Function\n",
    "\n",
    "The sigmoid function works fine for a \"yes or no\" problem, i.e. binary decisions. But more often than not we want to distinguish between more than two categories. For that, we need a function that takes in **multiple** neuron activations from the last layer of a network and outputs a **probability vector** containing the probabilities for each category. \n",
    "\n",
    "The key: **Each input** of this function is **normalized by the other inputs** such that the sum of the output vector is always 1. This activation function is different from ReLU or Sigmoid, because it always applies to the layer as a whole. In practice, it only makes sense as the activation function for the output layer.  Figure 3 shows an example network.\n",
    "\n",
    "We can realize a softmax activation function by taking each element $x_i$ of the input vector, calculating $\\exp(x_i)$ and then normalizing this value by dividing it by the sum of the $\\exp$ results of all single input vector elements. Strictly speaking, the $\\exp$ is not necessary for this effect - a linear normalization, limited to non-negative values, could also be interpreted as probability. However, the exponential normalization offers properties that improve performance (see \"further reading\").\n",
    "\n",
    "\\begin{align}\n",
    "(\\text{Softmax}(x))_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/softmax_example_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Softmax activation function\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pts):</b> In \"logistic regression\", we also obtained a probability by applying a sigmoid function on the last layers' output. Why can't we apply a sigmoid function on each output neuron of this network instead of a softmax and get a probability vector?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer: Wenn wir die sigmoid function auf jeden Neuron anwenden, bekommen wir als Summe aller Wahrscheinlichkeiten einen Wert > 1, wodurch wir keine Aussage zu der Klassifikation machen können. Anders gesagt, haben wir dann keinen Zusammenhang zwischen den einzelnen Neuronen-Wahrscheinlichkeiten, jedes Neuron bekommt seine Wahrscheinlichkeit unabhängig vom anderen. </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.6 Automated Classification Training\n",
    "\n",
    "### 2.6.1 Introduction\n",
    "\n",
    "We already have explored automated training using backpropagation in the last chapter. We had one set of points that we had to fit a function as close as possible. The task is similar for classification training. However instead of y-coordinates for points, we now have discrete categories.\n",
    "\n",
    "You got already a set of neck lengths and the correspoding categories (see table 1). In the field of machine learing, this dataset is called __training data__. It specifies the behaviour that the neural net should have. We will use backpropagation to adjust the weights and biases of the network over and over again until the network outputs the same values to a given set of inputs as in the training data. During backpropagation, the network is figuratively \"learning\" the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.6.2 Realizing an XOR Gate with a Neural Network\n",
    "\n",
    "You find yourself working as an engineer at a major electronic component manufacturing company. Your company wants to produce the first XOR gate chip that runs on artificial intelligence. You are given the training data in the form of a truth table:\n",
    "\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |0          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 2 - XOR Truth table\n",
    "</p>\n",
    "\n",
    "\n",
    "In this task we will make use of arrays and matrices to ease the handling of the data and the network parameters. We will also utilize a neural network without biases in order to make the algorithm as simple as possible.\n",
    "The training data consists of a 2D Array of all possible input states and a 1D Array of all corresponding outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.1 Task : Create Training Data\n",
    "\n",
    "A training set consists of an input set and a solution set. During supervised training, the network is adjusted until its predictions to the input set match the corresponding predetermined solutions.\n",
    "Complete the training data below using the truth table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Create Training Data. A training set consists of an input set and a solution set. During supervised training, the network is adjusted until its predictions to the input set match the corresponding predetermined solutions (not always see: Overfitting, but in this case). Complete the training data below using the groundtruth table above. Please initialize the solution '2 dimensional' as well.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_input_set: np.ndarray\n",
    "xor_solution_set: np.ndarray\n",
    "\n",
    "# STUDENT CODE HERE (1 pt)\n",
    "xor_input_set = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # Das sind die Punkte des XOR\n",
    "xor_solution_set = np.array([[0], [1], [1], [0]]) # Und das sind die Lösungen in der gleichen Reihenfolge wie in xor_input_set\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# Quick sanity check\n",
    "assert xor_input_set.shape == (len(xor_input_set),2), f'Expected shape of {(len(xor_input_set),2)}, but found {xor_input_set.shape}'\n",
    "assert xor_solution_set.shape == (len(xor_solution_set),1), f'Expected shape of {(len(xor_solution_set),1)}, but found {xor_solution_set.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.2 Initializing the Network\n",
    "Next, the Network has to be defined and initialized. For this task, we use a network with 3 hidden neurons (see Figure 4).\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/3x2_xor_network.png\" alt=\"Neural Network\" width=\"1000\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 4: Neural Network .</em></p>\n",
    "\n",
    "We define $w_{01}, w_{02}, w_{03}, w_{10}, w_{11}, w_{12}$ all at once by just defining a 2x3 weight matrix $w_{l1}$ and do the same for $w_{l2}$. The matrices will be initialized with values between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neural network class that is depicted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, in_dim : int, hl_dim : int, ol_dim : int):\n",
    "        self.in_dim = in_dim\n",
    "        self.hl_dim = hl_dim\n",
    "        self.ol_dim = ol_dim\n",
    "        self.hl_sum = [0] * in_dim\n",
    "        self.hl_activation = [0] * hl_dim\n",
    "        self.ol_sum = [0] * ol_dim\n",
    "        self.prediction = 0\n",
    "        self.b = 0\n",
    "        self.w_i = np.zeros((in_dim, hl_dim))\n",
    "        self.w_o = np.zeros((hl_dim, ol_dim))\n",
    "        self.history = []\n",
    "        \n",
    "    def set_conf(self, w_i: np.ndarray, w_o: np.ndarray, b: float):  # w_i and w_o are matrices here\n",
    "        assert w_i.shape == (self.in_dim, self.hl_dim)\n",
    "        assert w_o.shape == (self.hl_dim, self.ol_dim)\n",
    "        \n",
    "        self.w_i = w_i\n",
    "        self.w_o = w_o\n",
    "        self.b = b\n",
    "\n",
    "    def get_conf(self) -> Dict[str, Union[np.ndarray, float]]:\n",
    "        configuration = dict()\n",
    "        configuration['w_i'] = self.w_i\n",
    "        configuration['w_o'] = self.w_o\n",
    "        configuration['b'] = self.b\n",
    "        return configuration\n",
    "\n",
    "    def get_ex(self) -> Dict[str, float]:\n",
    "        excitations = dict();\n",
    "        excitations['hl_sum'] = self.hl_sum\n",
    "        excitations['hl_activation'] = self.hl_activation\n",
    "        excitations['ol_sum'] = self.ol_sum\n",
    "        return excitations\n",
    "    \n",
    "    \n",
    "    def show_conf(self):\n",
    "        print(\"weight matrix w_i:\")\n",
    "        print(self.w_i)\n",
    "        print(\"\\nweight matrix w_o:\")\n",
    "        print(self.w_o)\n",
    "        print(\"Bias\")\n",
    "        print(self.b)\n",
    "\n",
    "    def compute(self, input_set: np.ndarray) -> np.ndarray:\n",
    "        self.hl_sum = input_set.dot(self.w_i)\n",
    "        self.hl_activation = relu(self.hl_sum) \n",
    "        self.ol_sum = relu(self.hl_activation).dot(self.w_o) + self.b\n",
    "        self.prediction = sigmoid(self.ol_sum)\n",
    "\n",
    "        return self.prediction\n",
    "    \n",
    "    def save_configuration(self):\n",
    "        self.history.append(self.get_conf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to instantiate the neural network with the desired dimensions, before initializing it with random weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix w_i:\n",
      "[[0.12987473 0.79614992 0.9760545 ]\n",
      " [0.71622    0.40211837 0.52224639]]\n",
      "\n",
      "weight matrix w_o:\n",
      "[[0.98204208]\n",
      " [0.27249726]\n",
      " [0.51764363]]\n",
      "Bias\n",
      "1.9321132477960596\n"
     ]
    }
   ],
   "source": [
    "xor_logic_gate_net = NeuralNetwork(in_dim=2, hl_dim=3, ol_dim=1)\n",
    "\n",
    "def initialize_network(net):\n",
    "    #np.random.seed(3)\n",
    "    weight_matrix_i = np.random.rand(net.in_dim, net.hl_dim)  # a 2x3 matrix of weights\n",
    "    weight_matrix_o = np.random.rand(net.hl_dim, net.ol_dim)  # a 3x1 matrix of weights\n",
    "    bias = np.random.randn()\n",
    "    net.set_conf(weight_matrix_i,weight_matrix_o,bias)\n",
    "    \n",
    "initialize_network(xor_logic_gate_net) #just a test initialization to illustrate the weight matrices\n",
    "\n",
    "xor_logic_gate_net.show_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.3 Defining Training Process\n",
    "Finally, run the cells below to implement a backpropagation algorithm. Try to understand the code. See Fig. 4 for explanation of the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x: np.ndarray) -> np.ndarray: #the derivative of sigmoid\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: NeuralNetwork, input_set: np.ndarray, solution_set: np.ndarray, learning_rate: float, epochs: int):\n",
    "    for t in range(epochs):\n",
    "        \n",
    "        net.save_configuration()\n",
    "        # Forward pass: compute predicted solution_set\n",
    "        predictions = net.compute(input_set)\n",
    "        # Compute and print loss\n",
    "        log_loss = cross_entropy_loss(predictions, solution_set)\n",
    "        \n",
    "        if (t % 5 == 0):  # only output every 5th epoch\n",
    "            print(\"Loss after Epoch {}: {:0.4f}\".format(t, log_loss))\n",
    "\n",
    "        #unravel variables here for readability\n",
    "        ol_sum = net.get_ex()['ol_sum']\n",
    "        hl_activation = net.get_ex()['hl_activation']\n",
    "        hl_sum = net.get_ex()['hl_sum']\n",
    "        w_i = net.get_conf()['w_i']\n",
    "        w_o = net.get_conf()['w_o']\n",
    "        b = net.get_conf()['b']\n",
    "        \n",
    "        # Backpropagation to compute gradients of w_i and w_o with respect to loss\n",
    "        # start from the loss at the end and then work towards the front\n",
    "        grad_ol_sum = sigmoid_prime(ol_sum) * (predictions - solution_set)\n",
    "        grad_w_o = hl_activation.T.dot(grad_ol_sum)  # Gradient of Loss with respect to w_o\n",
    "        grad_hl_activation = grad_ol_sum.dot(w_o.T)  # the second layer's error\n",
    "        grad_hl_sum = hl_sum.copy()  # create a copy to work with\n",
    "        grad_hl_sum[hl_sum < 0] = 0  # the derivate of ReLU\n",
    "        grad_w_i = input_set.T.dot(grad_hl_sum * grad_hl_activation)  #\n",
    "\n",
    "        updated_weight_matrix_i = w_i - learning_rate * grad_w_i\n",
    "        updated_weight_matrix_o = w_o - learning_rate * grad_w_o\n",
    "        updated_bias = b - learning_rate * grad_ol_sum.sum()\n",
    "        net.set_conf(updated_weight_matrix_i, updated_weight_matrix_o,\n",
    "                       updated_bias)  # Apply updated weights to network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Choose Hyperparameters and Train\n",
    "<ul>\n",
    "<li> Choose an optimal learning rate and number of epochs by trying out values and running the cell below.\n",
    "<li> If your training data was correct, the network should be ready for use after training.\n",
    "A successfull training should result in a loss smaller than 0.02.\n",
    "                                                     \n",
    "<li><b>Hint:</b> Press Shift+Enter on the cell below and then the \"up\" arrow key to repeat the training easily.\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after Epoch 0: 0.7771\n",
      "Loss after Epoch 5: 0.7091\n",
      "Loss after Epoch 10: 0.5486\n",
      "Loss after Epoch 15: 0.4827\n",
      "Loss after Epoch 20: 0.0377\n",
      "Loss after Epoch 25: 0.0292\n",
      "Loss after Epoch 30: 0.0248\n"
     ]
    }
   ],
   "source": [
    "learning_rate: float\n",
    "epochs: int\n",
    "# STUDENT CODE HERE (2 pts)\n",
    "learning_rate = 10\n",
    "epochs = 35\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "initialize_network(xor_logic_gate_net) #initialize again so you can just run this box and train a new network\n",
    "train(xor_logic_gate_net, xor_input_set, xor_solution_set,learning_rate,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with a Contour Plot animation\n",
    "\n",
    "Now that you hopefully achieved promising results, we want to visualize the results with the help of a contour plot animation. The code to create them is located in the following cells. Run them and have a look at the animation. If it is not running properly, search it in your current directory. Ideally, you should see how the net narrows down its prediction to separate the classes from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(input_set: np.array):\n",
    "    '''Helper Function to create a numpy grid, which is feed into a neural net.'''\n",
    "    min_x = input_set[:,0].min()-3\n",
    "    max_x = input_set[:,0].max()+3\n",
    "    min_y = input_set[:,1].min()-3\n",
    "    max_y = input_set[:,1].max()+3\n",
    "\n",
    "    # create x and y base vectors\n",
    "    x_grid = np.arange(min_x, max_x, 0.1)\n",
    "    y_grid = np.arange(min_y, max_y, 0.1)\n",
    "\n",
    "    # create all of the lines and rows of the grid\n",
    "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "    # flatten each grid to a vector\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "\n",
    "    # horizontal stack vectors to create x1,x2 input for the model\n",
    "    grid = np.hstack((r1,r2))\n",
    "    return xx,yy,grid   \n",
    "\n",
    "def create_scatter(input_set: np.array, solution_set:np.array, ax: plt.Axes):\n",
    "    \"\"\"Helper function, which creates the scatter plot from a input_set and a solution_set\"\"\"\n",
    "    for class_value in range(2):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix, _ = np.where(class_value == solution_set)\n",
    "        \n",
    "        # create scatter of these samples\n",
    "        colors = np.array([\"red\", \"blue\"])\n",
    "        ax.scatter(input_set[row_ix, 0], input_set[row_ix, 1], c=colors[class_value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is actually obsolet, but usefull for a static contour plot \n",
    "def decision_boundary_plot(input_set: np.array, solution_set: np.array, model: NeuralNetwork):\n",
    "    '''Creates static contour plot'''\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xx, yy, grid = create_grid(input_set)\n",
    "\n",
    "    # make predictions for the grid\n",
    "    prediction = model.compute(grid)\n",
    "\n",
    "    # reshape the predictions back into a grid\n",
    "    zz = prediction.reshape(xx.shape)\n",
    "\n",
    "    # plot the grid of x, y and z values as a surface\n",
    "    contour_plot = ax.contourf(xx, yy, zz, cmap='RdBu')\n",
    "    fig.colorbar(contour_plot, ax= ax)\n",
    "\n",
    "    # create scatter plot for samples from each class\n",
    "    create_scatter(input_set, solution_set, ax)\n",
    "\n",
    "    ax.set_title(\"Contourplot\") \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Video\n",
    "\n",
    "def make_animation_of_net_history(history, input_set, solution_set, filename):\n",
    "    '''Creates animation from net history'''\n",
    "    fig, ax = plt.subplots()\n",
    "    # create grid for the predictions\n",
    "    xx, yy, grid = create_grid(input_set)\n",
    "\n",
    "    # full control over the colorbar, necessary for the animation\n",
    "    div = make_axes_locatable(ax)\n",
    "    cax = div.append_axes('right', '5%', '5%')  \n",
    "\n",
    "    plot_title = ax.set_title(\"\")\n",
    "\n",
    "    def animate(i):\n",
    "\n",
    "        # instantiate the net and load weights from history into it\n",
    "        net = NeuralNetwork(in_dim=2, hl_dim=3, ol_dim=1)\n",
    "        w_i = history[i][\"w_i\"]\n",
    "        w_o = history[i][\"w_o\"]\n",
    "        b = history[i][\"b\"]\n",
    "        net.set_conf(w_i, w_o, b)\n",
    "        prediction = net.compute(grid)\n",
    "        zz = prediction.reshape(xx.shape)\n",
    "\n",
    "        # reset colorbar axes\n",
    "        cax.cla()\n",
    "        \n",
    "        contour_plot = ax.contourf(xx, yy, zz, cmap='RdBu')\n",
    "        fig.colorbar(contour_plot, cax= cax)\n",
    "\n",
    "        create_scatter(input_set, solution_set, ax)\n",
    "\n",
    "        plot_title.set_text(f\"Epoch: {i}\")\n",
    "        \n",
    "        return contour_plot, plot_title\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=len(history), blit=True)\n",
    "    ani.save(filename)\n",
    "\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"XOR_contourplot.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "make_animation_of_net_history(xor_logic_gate_net.history, xor_input_set, xor_solution_set, \"XOR_contourplot.mp4\")\n",
    "Video('XOR_contourplot.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. Why are the losses different each time you run the cell?<br>\n",
    "2. What is a good learning rate that reaches a loss < 0.02 in < 100 epochs most of the time?<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. Weil wir bei jedem Start unterschiedliche Anfangswerte (Parameter) haben. Und diese Werte sind random gewählt. <br>\n",
    "2. Mit learning_rate = 10 und epochs = 35 erreichen wir Loss < 0.02 <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Classification Test. Run the cell below and change the sliders and do a validation check on your logic gate.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ce448f8fd2424a8bd25c5b16ab50a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', layout=Layout(width='22%'), max=1.0, step=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def change(input1: float, input2: float):\n",
    "    input_vector = np.array([input1 * 1, input2 * 1])     # converting bool to float\n",
    "    prediction = xor_logic_gate_net.compute(input_vector)\n",
    "    print(\"\\t input: {} \\t \\t output: {:0.9f}\".format(input_vector, prediction[0]))\n",
    "\n",
    "interact(\n",
    "    change,\n",
    "    input1=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    "    input2=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Continuous Input Test. Change the sliders and observe the changes when the input is varied continuously instead of binary.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0a75d3ddc24b81b5f0b23899c67cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', max=1.0), FloatSlider(value=0.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(change, input1=0.0, input2=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (5 pts):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to:<br>\n",
    "    \n",
    "1. What can you observe when changing the sliders? How would you describe the general relationship between the two inputs and the output (a few words)<br>\n",
    "2. Change the sliders to the training data values e.g.(1.00, 1.00). Does the output match the training data exactly? Why is that the case?<br>\n",
    "3. The neural network now can do something more than just predicting the values of the input set that you gave it. What \"special ability\" has your network gained automatically? (Hint: Think about neural networks in general, the XOR gate is just an example)<br>\n",
    "4. How can this special ability be useful when applying neural networks to self-driving vehicles?<br>\n",
    "5. Why does this ability make it easier to use a neural network for self-driving vehicles than traditional rule-based programming. (One pos. and neg. aspect)<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. Immer wenn sich die Inputs vom Wert her annähren, nährt sich der Output zu 0. Und das gegenteil auch<br>\n",
    "2. Nein, weil wir erstens eine Approximation haben und keine Exakte Vorhersage! Wir wollen nämlich die Muster lernen und nicht auswendig die Antworten kennen. zweitens gibt uns das NN einen Kontinuierlichen Wert für die Approximation obwohl die Trainingsdaten diskret waren.  <br>\n",
    "3. Generalisieren!! Wir haben jetzt einen kontinuierlichen Verlauf für die Vorhersage. Für Fälle wie (0.1,0.5) kann jetzt mein Modell eine Vorhersage machen obwohl er nicht auf solche gelernt wurde. <br>\n",
    "4. Zum Beispiel : Das Modell trainiert bei Sonne → funktioniert auch bei Regen, Nebel, Schnee, trainiert auf Autobahnen → funktioniert auch auf Landstraßen, trainiert mit roten Autos → erkennt auch blaue, grüne Autos, etc. <br>\n",
    "5. Weil wir beim traditionellen Programmieren, explizit alle Regeln erwähnen müssen (Mio. von Regeln) was praktisch nicht machbar ist. Und bei NN haben wir das Generalisieren!  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Create an OR gate:\n",
    "<ul>\n",
    "<li> Adjust the solution set.\n",
    "<li> Choose hyperparameters and train.\n",
    "<li> Verify your results with a simple test. This is up to you.\n",
    "\n",
    "<li><b>Hint:</b> Have a look at the truth table of the OR gate below. \n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |1          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 3 - OR Truth table\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input set stays the same, but we assign it a new name for code clarity\n",
    "or_input_set = xor_input_set.copy()\n",
    "\n",
    "# STUDENT CODE HERE (1 pt)\n",
    "or_solution_set = np.array([[0], [1], [1], [1]])\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after Epoch 0: 0.5009\n",
      "Loss after Epoch 5: 0.4886\n",
      "Loss after Epoch 10: 0.4772\n",
      "Loss after Epoch 15: 0.4665\n",
      "Loss after Epoch 20: 0.4562\n",
      "Loss after Epoch 25: 0.4464\n",
      "Loss after Epoch 30: 0.4368\n",
      "Loss after Epoch 35: 0.4274\n",
      "Loss after Epoch 40: 0.4182\n",
      "Loss after Epoch 45: 0.4092\n"
     ]
    }
   ],
   "source": [
    "or_logic_gate_net = NeuralNetwork(in_dim = 2, hl_dim=3, ol_dim=1)\n",
    "\n",
    "learning_rate: float\n",
    "epochs: int\n",
    "# STUDENT CODE HERE (2 pts)\n",
    "learning_rate = 0.1\n",
    "epochs = 50\n",
    "# STUDENT CODE until HERE\n",
    "initialize_network(or_logic_gate_net)\n",
    "train(or_logic_gate_net, or_input_set, or_solution_set,learning_rate,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"OR_contourplot.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_animation_of_net_history(or_logic_gate_net.history, or_input_set, or_solution_set, \"OR_contourplot.mp4\")\n",
    "Video(\"OR_contourplot.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cfc50489bd4bed86a21abfb7f355ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', layout=Layout(width='22%'), max=1.0, step=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write a short test in this code block\n",
    "# STUDENT CODE HERE (2 pts)\n",
    "\n",
    "def change(input1: float, input2: float):\n",
    "    input_vector = np.array([input1 * 1, input2 * 1])\n",
    "    prediction = or_logic_gate_net.compute(input_vector)\n",
    "    print(\"\\t input: {} \\t \\t output: {:0.9f}\".format(input_vector, prediction[0]))\n",
    "    \n",
    "interact(\n",
    "    change,\n",
    "    input1=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    "    input2=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    ");\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Neural-Networks using DeepLearningLibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]], dtype = 'float64')\n",
    "\n",
    "y_train = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]], dtype = 'float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 PyTorch Example\n",
    "This example is just a reference for how the syntax will look when using PyTorch. You do not need to install PyTorch just to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop:\n",
      "Epoch:        0  |  Loss: 0.7773979902267456\n",
      "Epoch:       50  |  Loss: 0.6953570246696472\n",
      "Epoch:      100  |  Loss: 0.6885163187980652\n",
      "Epoch:      150  |  Loss: 0.6886417269706726\n",
      "Epoch:      200  |  Loss: 0.6892629861831665\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3, True)\n",
    "        self.fc2 = nn.Linear(3, 1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "inputs = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "targets = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training loop:\")\n",
    "for idx in range(0, 201):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    if idx % 50 == 0:\n",
    "        print(\"Epoch: {: >8}  |  Loss: {}\".format(idx, loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Outlook: Classification Tests in the Real World\n",
    "\n",
    "A classic application of neural networks is the classification of images. A commonly used data set is CIFAR-10, which consists of:  \n",
    " 1. Images of  airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks  (10 Categories)\n",
    " 2. Labels attached to each image that categorize the image\n",
    " \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/cifar10_plot.png\" alt=\"CIFAR-10 dataset[4]\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 3: CIFAR-10 dataset[4]</em></p>\n",
    "\n",
    " \n",
    "The labels (also called annotations) act as the \"solution\" for the training set. Each item (airplane, car..) is a separate category. \n",
    "During training, the weights and biases in the network are adjusted in just the right way, until it performs the right mathematical operations to correctly classify the given training data. After training, the network can recognize whether the image is a cat, an airplane, etc. This even works for pictures that the network has never seen. You will find out how neural networks can perform image classification in the next class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "[1] Wikipedia, Statistical classification https://en.wikipedia.org/wiki/Statistical_classification, retrieved 01.05.2019\n",
    "\n",
    "[2]  Brownlee, Jason 2018. Machine Learning Algorithms From Scratch. p. 70\n",
    "\n",
    "[3]  Gibbs, M.N. (Nov 2000). \"Variational Gaussian process classifiers\". IEEE Transactions on Neural Networks. p. 1458–1464.\n",
    "\n",
    "[4] Cifar-10, Cifar-100 Dataset Introduction\n",
    "Corochann - https://corochann.com/cifar-10-cifar-100-dataset-introduction-1258.html, retrieved 02.02.2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "The Sigmoid Function in Logistic Regression: http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/\n",
    "\n",
    "Why Softmax uses exponential function: https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback and Recap\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3pt):</b>  Please conclude in a few sentences what you learned in this exercise\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer: I have learned how neural networks really work and how they solve the classification problem, for both binary and multi-class classification with different activation functions.</b> \n",
    "</div>\n",
    "\n",
    "## And give us feedback if you like\n",
    "\n",
    "\n",
    "1) Do you think this task was designed well? \n",
    "\n",
    "2) Where can we improve this task?\n",
    "\n",
    "<strong>Thanks for participating in LAMA! :)</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama25-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "395.996px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
