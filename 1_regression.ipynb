{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Group Number:</b> 7\n",
    "<br><b>Name Group Member 1:</b>   Paraa Afifi\n",
    "<br><b>u-Kürzel Group Member 1:</b>   uppns\n",
    "<br><b>Name Group Member 2:</b>   Dan-Jason Bräuninger\n",
    "<br><b>u-Kürzel Group Member 2:</b>   uuuab\n",
    "<br><b>Name Group Member 3:</b> Sami Shahzad\n",
    "<br><b>u-Kürzel Group Member 3:</b>uvoei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression with Neural Networks\n",
    "\n",
    "Neural networks are used in a wide variety of applications—ranging from regression and classification tasks to advanced sequence modeling. In this section, we focus on the fundamentals of **regression** using neural networks. We will illustrate these foundations with simple examples that cover:\n",
    "\n",
    "1. The concept of a single artificial neuron.\n",
    "2. Activation functions and how they add non-linearity.\n",
    "3. A brief introduction to backpropagation.\n",
    "\n",
    "We will build up from the simplest neuron (linear neuron) to networks with multiple neurons that enable us to approximate more complex functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Artificial Neuron (Theory)\n",
    "\n",
    "An **artificial neuron** is a mathematical function inspired by the way biological neurons process information. It can have one or more inputs ($x_n$) and produces a single output (y). The neuron applies a simple mathematical operation, as shown below:\n",
    "\n",
    "$$\n",
    "  f_{\\text{neuron}}(x) = \\phi\\left(\\sum_{n=1}^m x_n \\, w_{n} \\;+\\; b\\right)\n",
    "\\quad (1)\n",
    "$$\n",
    "\n",
    "- **Inputs**: $x_n$ are numeric values from data or other neurons.\n",
    "- **Weights**: $w_n$ scale the importance of each input.\n",
    "- **Summation**: The weighted inputs are summed up into an intermediate value $v$.\n",
    "- **Bias** $b$: A constant added to the sum.\n",
    "- **Activation Function** $\\phi$: A possibly non-linear function applied to the sum.\n",
    "- **Output** $y$: The final activation of the neuron.\n",
    "\n",
    "Even though a single neuron only performs a simple transformation, large networks of these neurons can solve very complex tasks.\n",
    "\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/neural_network.png\" alt=\"A diagram illustrating a single neuron\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 1: General artificial neuron.</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "\n",
    "- An artificial neuron is an **abstract** concept. Different software or hardware approaches (even \n",
    "  [optical computing](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-6-9-1132)) \n",
    "  can implement the same mathematical function in (1).  \n",
    "\n",
    "- Sometimes the bias is treated as if it were another weight $w_0$ with a fixed input $x_0 = 1$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A Simple Neuron (Practice)\n",
    "\n",
    "Let us consider the simplest case: a single neuron with **one input** and **no activation function**. This neuron essentially computes:\n",
    "\n",
    "$$\n",
    "  f_{\\text{neuron}}(x) = w \\cdot x + b \n",
    "\\quad (2)\n",
    "$$\n",
    "\n",
    "This is simply a linear (or affine) function. Graphically, it looks like:\n",
    "\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/single_neuron_no_activation.png\" alt=\"A diagram illustrating a single neuron\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 2: Simple artificial neuron without activation.</em></p>\n",
    "\n",
    "In practice, we will:\n",
    "- Keep track of `weight` and `bias`.\n",
    "- Compute the neuron’s output based on input $x$ as $w \\cdot x + b$.\n",
    "\n",
    "Below, we define a Python class `SimpleNeuron` that:\n",
    "- Holds a single weight and a single bias.\n",
    "- Has methods to set or get these parameters.\n",
    "- Provides a `compute` method to evaluate $f_{\\text{neuron}}(x)$.\n",
    "- Links to an interactive plot that updates whenever we change the neuron’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, List, Tuple\n",
    "import numpy as np\n",
    "from __future__ import annotations # Used to allow referencing classes that have not yet been defined in type annotations. This will become default behaviour in Python 3.10. Until then, we have to use this line to enable that behaviour\n",
    "\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight: float, bias: float):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self) -> float:\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self) -> float:\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.activation = np.dot(self.weight, x) + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The Problem of Regression\n",
    "\n",
    "We use **regression** to approximate a function that fits a given set of data points as accurately as possible. A common measure of “fit” is the **mean squared error (MSE)**:\n",
    "\n",
    "$$\n",
    "  J = \\frac{1}{N} \\sum_{n=1}^{N} \\bigl(\\hat{f}(x_n) - y_n\\bigr)^2,\n",
    "\\quad (3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{f}(x_n)$ is the model’s predicted output for an input $x_n$,\n",
    "- $y_n$ is the actual target value,\n",
    "- $N$ is the number of data points.\n",
    "\n",
    "We aim to minimize $J$. A lower $J$ indicates that our model’s predictions are closer to the target values. Once we have found a good approximation, we can use the trained model to **predict** new points by providing new $x$-values.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/least_squares_explanation.png\" alt=\"Visualization of least squares approach\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 3: Illustration of distances between data points and a model’s curve.</em></p>\n",
    "\n",
    "<a id='simple_neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "\n",
    "- In equation (3), $x_1, x_2, \\dots, x_N$ represent **samples** (not neuron inputs like in equation (1)).\n",
    "- Our goal is to choose weights and biases to minimize $J$.\n",
    "- The **mean squared error** is based on a distance metric, and can be replaced by other metrics which may fit the target problem better.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function \"loss\" that performs the operation (3). It will receive a neuron object and a set of points as arguments.\n",
    "- For each point that we give it, it first separates x and y-values. \n",
    "- It hands the neuron an x-value and asks the neuron to compute a prediction for the y-value. (see $\\hat{f}(x_n)$) \n",
    "- Then it subtracts the real y-value from the predicted y-value, as in operation (3), resulting in a distance\n",
    "- It then squares up the distance and accumulates the squared distances.  \n",
    "- In the last step, it divides the sum of squared distances by the amount of compared points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> float:\n",
    "    sum_squared_dist = 0\n",
    "\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # zip merges both points[\"x\"] and points[\"y\"]\n",
    "\n",
    "        predicted_point_y = neuron.compute(point_x)\n",
    "        dist = point_y - predicted_point_y\n",
    "        squared_dist = dist ** 2\n",
    "        sum_squared_dist += squared_dist\n",
    "\n",
    "    loss = sum_squared_dist / len(points[\"y\"])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3.1 Preparing an Interactive Plot\n",
    "\n",
    "Below, we define a helper class for interactive plotting:\n",
    "- It can register a neuron and update its plot when the neuron’s parameters change.\n",
    "- We will display **data points** alongside the **neuron’s predicted line**.\n",
    "- We will also define a `loss` function to compute the mean squared error $J$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The plot classes are not part of the subject matter for this lab.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, Layout, HBox, FloatSlider\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], width: int = 800, height: int = 400, margin: Dict[str, int] = {'t': 0, 'l': 170}, draw_time: float = 0.05):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.1)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.1)\n",
    "        self.draw_time = draw_time\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Input: x\", range=ranges[\"x\"], fixedrange=True),\n",
    "            yaxis=dict(title=\"Output: y\", range=ranges[\"y\"], fixedrange=True),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "        self.plot_points = go.Scatter(\n",
    "            x=points[\"x\"], y=points[\"y\"], mode=\"markers\")\n",
    "        self.data = [self.trace, self.plot_points]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron) -> None:\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self) -> None:\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self) -> None:\n",
    "        print(\"Loss: {:0.2f}\".format(loss(self.neuron, self.points)))\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Train the neuron\n",
    "<ul>\n",
    "<li> You are given a set of 3 points and one neuron to do a curve fit. Run the cell below.\n",
    "<li> <b>Change the weight and bias of the neuron using the sliders to minimize the loss.</b>\n",
    "    <li><b>Hint:</b> You can also change the sliders with the arrow keys on your keyboard after clicking on the slider.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a28f7d9960463ca678c28792c901f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=3.0, min=-3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4badda5e95644eba9c0e1ebcd5471bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '78fd7b5e-c06b-4193-87ee-0a3cec754e28',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmQ1AdmZmZmZmDkBDMzMzMzMPQA=='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmQ1AdmZmZmZmDkBDMzMzMzMPQA=='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '95ca5dbd-fde4-43f1-a5a4-ba01c3e30cb8',\n",
       "              'x': [1, 2, 3],\n",
       "              'y': [1.5, 0.7, 1.2]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 4], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 4], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_linreg = dict(x=[1, 2, 3], y=[1.5, 0.7, 1.2])\n",
    "ranges_linreg = dict(x=(-4, 4), y=(-4, 4))\n",
    "\n",
    "linreg_plot = Interactive2DPlot(points_linreg, ranges_linreg)\n",
    "simple_neuron = SimpleNeuron(linreg_plot)\n",
    "\n",
    "slider_layout = Layout(width=\"90%\")\n",
    "\n",
    "interact(\n",
    "    simple_neuron.set_values, \n",
    "    weight=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout)\n",
    ")\n",
    "\n",
    "linreg_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What is the optimal weight and bias combination? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    <ul>\n",
    "        <li> weight ~= -0,2 or -0,1</li>\n",
    "        <li> bias ~= 1,50 or 1,40</li>\n",
    "        <li> loss = 0,10 = 0,10</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 3D Visualization of Loss Surface\n",
    "\n",
    "We can also visualize the incurred loss $J$ based on our parameters in 3D space:\n",
    "- The **x-axis** will represent the weight.\n",
    "- The **y-axis** will represent the bias.\n",
    "- The **z-axis** (height) represents $\\log_{10}$ of the loss (for easier visibility).\n",
    "- A movable **black sphere** shows the loss for the current $(w, b)$.\n",
    "\n",
    "This gives us a “surface” where the minimum is the bottom of a valley.  This can be though of as the **optimum** weight and bias combination, which minimizes the loss and ideally fits the data best.\n",
    "\n",
    "Below, you can interact with both a 3D surface plot of $\\log(\\text{MSE})$ and a 2D plot of the neuron’s line vs. data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> np.ndarray:\n",
    "    least_squares_loss = loss(neuron, points)\n",
    "    return np.log10(least_squares_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interactive3DPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], width: int = 600, height: int = 600, draw_time: float = 0.1):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.draw_time = draw_time\n",
    "        self.threading = threading\n",
    "\n",
    "        self.range_weights = np.arange(  # Array with all possible weight values in the given range\n",
    "            ranges[\"x\"][0], ranges[\"x\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases = np.arange(  # Array with all possible bias values in the given range\n",
    "            ranges[\"y\"][0], ranges[\"y\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases_t = self.range_biases[:, np.newaxis]  # Bias array transposed\n",
    "        self.range_losses = []  # initialize z axis for 3D surface\n",
    "\n",
    "        self.ball = go.Scatter3d(  # initialize ball\n",
    "            x=[], y=[], z=[], hoverinfo=\"none\", mode=\"markers\", marker=dict(size=12, color=\"black\")\n",
    "        )\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=dict(t=0, l=0),\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"Weight\", range=ranges[\"x\"], autorange=False, showticklabels=True),\n",
    "                yaxis=dict(title=\"Bias\", range=ranges[\"y\"], autorange=False, showticklabels=True),\n",
    "                zaxis=dict(title=\"Loss: log(MSE)\", range=ranges[\"z\"], autorange=True, showticklabels=False),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.data = [\n",
    "            go.Surface(\n",
    "                z=self.range_losses,\n",
    "                x=self.range_weights,\n",
    "                y=self.range_biases,\n",
    "                colorscale=\"Viridis\",\n",
    "                opacity=0.9,\n",
    "                showscale=False,\n",
    "                hoverinfo=\"none\",\n",
    "            ),\n",
    "            self.ball,\n",
    "        ]\n",
    "\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "        self.calc_surface()\n",
    "\n",
    "        # height of 3d surface represents loss of weight/bias combination\n",
    "        # In the 2D plot, x is an array from e.g. -4 to +4. But the weights and biases only have a single value\n",
    "        # Here x will be the points to do regression and to calculate the loss on. \n",
    "        # The surface is spanned by the arrays of weight and bias.\n",
    "        \n",
    "    def calc_surface(self):  \n",
    "                \n",
    "        self.neuron.weight = (  #instead of 1 weight and 1 bias, let Neuron have an array of all weights and biases\n",
    "            self.range_weights\n",
    "        )\n",
    "        self.neuron.bias = self.range_biases_t\n",
    "        self.range_losses = log_mse(  # result: matrix of losses of all weight/bias combinations in the given range\n",
    "            self.neuron, self.points\n",
    "        )\n",
    "        self.plot.data[0].z = self.range_losses\n",
    "\n",
    "    def update(self):\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()\n",
    "\n",
    "    def redraw(self):  # when updating, only the ball is redrawn\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.ball.x = [self.neuron.weight]\n",
    "        self.ball.y = [self.neuron.bias]\n",
    "        self.ball.z = [log_mse(self.neuron, self.points)]\n",
    "        self.plot.data[1].x = self.ball.x\n",
    "        self.plot.data[1].y = self.ball.y\n",
    "        self.plot.data[1].z = self.ball.z\n",
    "        self.idle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges_3d: Dict[str, Tuple[float, float]], ranges_2d: Dict[str, Tuple[float, float]]):\n",
    "        self.plot_3d = Interactive3DPlot(points, ranges_3d)\n",
    "        self.plot_2d = Interactive2DPlot(points, ranges_2d, width=400, height=500, margin=dict(t=200, l=30))\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.plot_3d.register_neuron(neuron)\n",
    "        self.plot_2d.register_neuron(neuron)\n",
    "\n",
    "    def update(self):\n",
    "        self.plot_3d.update()\n",
    "        self.plot_2d.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Train the neuron\n",
    "<ul>\n",
    "<li> You are given the same set of 3 points and again one neuron to do a curve fit. Run the cell below.\n",
    "<li> <b>Change the weight and bias of the neuron using the sliders to minimize the loss.</b>\n",
    "<li> <b>Observe all changes.</b>\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can turn the 3D-Plot by clicking on it and moving your cursor, but you have to stay inside the widget with your cursor. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5959b1d386a446d4bac4f04836b12011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=2.0, min=-2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688b0e3e73054ee080c525cc21d336a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges_3d = dict(x=(-2.5, 2.5), y=(-2.5, 2.5), z=(-1, 2.5))  # set up ranges for the 3d plot\n",
    "plot_task2 = DualPlot(points_linreg, ranges_3d, ranges_linreg)  # create a DualPlot object to mange plotting on two plots\n",
    "neuron_task2 = SimpleNeuron(plot_task2)  # create a new neuron for this task\n",
    "\n",
    "interact(\n",
    "    neuron_task2.set_values,\n",
    "    weight=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    ")\n",
    "\n",
    "HBox((plot_task2.plot_3d.plot, plot_task2.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> In general, what does the optimal weight and bias combination correspond to in the 3D Plot? And what is the steepness in this point?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> Die beste Kombi für den kleinsten Loss ist der Globale Minimum. An diesem Punkt erreichen wir den kleinsten Wert der Loss-Funktion und liegen somit am nächsten zum Istwert. Die Steilheit an diesem Punkt ist gleich NULL -> Das ist genau der Punkt, an dem der Gradient in beide Richtungen ( Weight und Bias ) gleich Null ist. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activation Functions\n",
    "\n",
    "So far, our neuron was purely linear. To capture more complex relationships, we need **non-linear activation functions**. Without them, a deep network reduces to a $\\textbf{single-layer linear model}$, as a composition of linear functions remains linear. This severely limits the network's ability to handle $\\textbf{non-linear problems}$, As noted in $\\textit{Deep Learning}$ ([Goodfellow et al.](https://www.deeplearningbook.org/), 2016, p. 168).   \n",
    "\n",
    "One popular choice is the **Rectified Linear Unit (ReLU)**:\n",
    "$$\n",
    "  \\phi_{\\mathrm{ReLU}}(x) = \\max(0, x).\n",
    "\\quad (4)\n",
    "$$\n",
    "\n",
    "This simple function outputs 0 for negative inputs and behaves linearly for positive inputs. This non-linearity allows networks to approximate more sophisticated functions than a pure linear model. \n",
    "\n",
    "Below, we define `relu` and then illustrate a `ReluNeuron` class that inherits from `SimpleNeuron` but applies $\\max(0, x)$ to the output.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/single_neuron_relu.png\" alt=\"Diagram of a neuron with ReLU activatio\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 5: Neuron with ReLU activation.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input_val: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(input_val, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b>  Implement an artificial neuron with relu activation function\n",
    "<ul>\n",
    "<li> Complete artificial neuron code below by using the relu function from above to calculate its activation, like in Figure 5. </li>\n",
    "<li>Take a look at the <a href=\"#simple_neuron\">Simple Neuron Class</a> and appropriately update the compute method for the ReluNeuron with the just-defined relu function.</li>\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNeuron(SimpleNeuron): #inherit from SimpleNeuron class\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        # STUDENT CODE HERE (1 pt)\n",
    "        '''\n",
    "        *) Was wir quasi hier machen ist, mit der Funktion super() erben wir die Funktion compute() von der Elternklasse SimpleNeuron\n",
    "        *) Die Funktion compute() hat die Aufgabe, den linearen Zusammenhang auszurechnen für ein gegebenes weight und Bias -> und die x-Eingabe wird als parameter in compute(x) eingegeben.\n",
    "        *) Da wir jetzt die Aktivierungsfunktion relu() anwenden wollen und das lokal auf jedem linearen Zusammenhang, um eine nicht-Linearität einzuführen, wenden wir die Funktion auf der Ausgabe von compute() an. \n",
    "        '''\n",
    "        Ausgabe = super().compute(x)\n",
    "        self.activation = relu(Ausgabe)\n",
    "        # STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Task: Nonlinear Climate Control\n",
    "\n",
    "Imagine you work at \"ClimaTronics\", and need to design an AI-based climate control system with the following requirements:\n",
    "- **Climate control off** for temperatures under 25°C.\n",
    "- **At 30°C**, it should reach **10%** of its cooling power.\n",
    "- **Between 30°C and 40°C**, cooling power rises **quadratically** with temperature.\n",
    "- **At 40°C**, cooling power is **100%** (the maximum).\n",
    "\n",
    "These points form a non-linear curve as per Figure 6. We’ll attempt to approximate it with a **single ReLU neuron**.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/datasheet.png\" alt=\"ClimaTronics target curve\" width=\"750\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 6: ClimaTronics target curve.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffa066e944041a9af30624acbdd6a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=10.0, min=-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86550d0ce4c4ae39ceb67b852afd714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '38b19c61-0cef-4fc4-b163-3d9ef449e82e',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmVlGQGxmZmZmZkZAOTMzMzNzRkA='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'zMzCxaQDozMzMzM1pAoJmZmZk5WkA='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ccddb6fe-ffec-4844-9819-cd92eab7d9fc',\n",
       "              'x': [25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0],\n",
       "              'y': [0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 45], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 105], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_climate = dict(x=[25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0], y=[0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0])\n",
    "\n",
    "ranges_climate = dict(x=(-4, 45), y=(-4, 105))\n",
    "climate_plot = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_relu_neuron = ReluNeuron(climate_plot)\n",
    "\n",
    "interact(\n",
    "    our_relu_neuron.set_values,\n",
    "    weight=FloatSlider(min=-10, max=10, step=0.1, value=0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-200.0, max=200.0, step=1, value=0, layout=slider_layout),\n",
    ")\n",
    "\n",
    "climate_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. When setting the bias to 0.00, how does changing the weight affect the output function? <br>\n",
    "2. How does changing the bias affect the output function? <br>\n",
    "3. When setting the weight to 1.00 and the bias to -10, at what temperature does the climate control start? <br>\n",
    "4. When setting the weight to 1.00 and the bias to -20, at what temperature does the climate control start? <br>\n",
    "5. When setting the weight to 2.00 and the bias to -20, at what temperature does the climate control start? <br>\n",
    "6. What's the best weight/bias configuration that you could find? <br>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. <br> Der weight Parameter ist hauptsächlich dafür da, die Steigung des Anstiegs zu bestimmen -> Bei einer positiven Steigung ist für x < 0 alles gleich NULL und bei einer negativen Steigung ist für x > 0 alles gleich NULL. \n",
    "2. <br> Der Bias Parameter ist hauptsächlich dafür da, den \"Knick-Punkt\" auf der x-Achse zu verschieben (Rechts-Links)\n",
    "3. <br> In diesem Fall ist der \"Knick-Punkt\" bei 9.8, d.h. wie Klimaanlage fängt schon bei 9.8C an, aufzuheißen.\n",
    "4. <br> In diesem Fall ist der \"Knick-Punkt\" bei 19.9, d.h. wie Klimaanlage fängt schon bei 19.9C an, aufzuheißen.\n",
    "5. <br> In diesem Fall ist der \"Knick-Punkt\" bei 9.9, d.h. wie Klimaanlage fängt schon bei 9.9C an, aufzuheißen.\n",
    "6. <br> Wir wollen ja eine Konfiguration bei der die Loss-Funktion am niedrigsten ist, diese ist genau bei weight = 7.1, bias = -199 zu finden mit einem Loss von 50,59\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Neural Networks\n",
    "\n",
    "Our singular neuron, couldn't capture the desired quadratic scaling. To achieve more complex approximations, we can combine multiple neurons:\n",
    "- Multiple **hidden neurons** allow for multiple “bends” or “segments” in the overall function.\n",
    "- Stacking layers of neurons forms a *Multi-Layer Perceptron (MLP)*.\n",
    "\n",
    "Below, we show an example network with **two ReLU neurons** in the hidden layer and **one output neuron** (which, in this example, is linear). We can choose the weights and biases freely, but the resulting function is more flexible than a single neuron.\n",
    "<center>\n",
    "  <img src=\"images/hidden_layer.png\" alt=\"Diagram of a hidden layer with two neurons\" width=\"700\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 6: A neural network with one hidden layer (two neurons) and one linear output neuron.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "\n",
    "- For simplicity and reusability, we treat neural networks like individual neurons. Since a neuron is just a mathematical function, an entire network can also be represented as a single function, as shown in the activation calculation—without requiring explicit neuron objects.\n",
    "-  With at least one hidden layer and a suitable non-linear activation function, a neural network can theoretically approximate any continuous function. More about this can be read in \"Further Reading\" at the end of the notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Building a Simple Network\n",
    "\n",
    "Below, we define a minimal Python class for a 2-neuron hidden layer plus 1-neuron output:\n",
    "- Weights: $w_{i1}, w_{o1}, w_{i2}, w_{o2}$\n",
    "- Biases: $b_1, b_2$\n",
    "- The output (as per Figure 6) is:\n",
    "$$\n",
    "   \\text{network\\_output}(x) \n",
    "   = \\mathrm{ReLU}(w_{i1} x + b_1) \\cdot w_{o1}\n",
    "   + \\mathrm{ReLU}(w_{i2} x + b_2) \\cdot w_{o2}.\n",
    "$$\n",
    "\n",
    "We connect it to the same interactive plotting scheme. Again, you will be able to move sliders for these **six parameters** to see how the curve changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_config(self, w_i1: float, w_o1: float, b1: float, w_i2: float, w_o2: float, b2: float):\n",
    "        self.w_i1 = w_i1\n",
    "        self.w_o1 = w_o1\n",
    "        self.b1 = b1\n",
    "        self.w_i2 = w_i2\n",
    "        self.w_o2 = w_o2\n",
    "        self.b2 = b2\n",
    "        self.show_config()\n",
    "        self.plot.update()  # please redraw my output\n",
    "\n",
    "    def show_config(self):\n",
    "        print(\"w_i1:\", self.w_i1, \"\\t| \", \"w_o1:\", self.w_o1,\"\\n\")\n",
    "        print(\"b1:\", self.b1, \"\\t| \", \"w_i2:\", self.w_i2,\"\\n\")\n",
    "        print(\"w_o2:\", self.w_o2, \"\\t| \", \"b2:\", self.b2,\"\\n\")\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.prediction = (relu(self.w_i1 * x + self.b1) * self.w_o1\n",
    "                         + relu(self.w_i2 * x + self.b2) * self.w_o2)\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Task: Nonlinear Climate Control with a Small Neural Network\n",
    "\n",
    "This time, let’s approximate the same climate-control curve (25–40°C) with **two ReLU neurons** in the hidden layer:\n",
    "- Each hidden neuron can provide one “bend” in the function.\n",
    "- The output is a linear combination of those two ReLU outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe3c2ffc35b45889307d797fcfa16fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w_i1', layout=Layout(width='90%'), max=10.0, min=-10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d93425278e4447fb0078be5a97da831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '7ae132ff-cab6-4a83-8730-01a9e641e4e8',\n",
       "              'x': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'mZmVlGQGxmZmZmZkZAOTMzMzNzRkA='),\n",
       "                    'dtype': 'f8'},\n",
       "              'y': {'bdata': ('AAAAAAAAEMAzMzMzMzMPwGZmZmZmZg' ... 'zMzCxaQDozMzMzM1pAoJmZmZk5WkA='),\n",
       "                    'dtype': 'f8'}},\n",
       "             {'mode': 'markers',\n",
       "              'type': 'scatter',\n",
       "              'uid': '5502efa6-74d2-4764-bfd6-6da2c3995c20',\n",
       "              'x': [25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0],\n",
       "              'y': [0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0]}],\n",
       "    'layout': {'autosize': False,\n",
       "               'height': 400,\n",
       "               'margin': {'l': 170, 't': 0},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'width': 800,\n",
       "               'xaxis': {'fixedrange': True, 'range': [-4, 45], 'title': {'text': 'Input: x'}},\n",
       "               'yaxis': {'fixedrange': True, 'range': [-4, 105], 'title': {'text': 'Output: y'}}}\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_plot_adv = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_neural_net = NeuralNetwork(climate_plot_adv)\n",
    "\n",
    "interact(\n",
    "    our_neural_net.set_config,\n",
    "    w_i1=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o1=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b1=FloatSlider(min=-200.0, max=200.0, step=1,  layout=slider_layout),\n",
    "    w_i2=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o2=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b2=FloatSlider(min=-200.0, max=200.0, step=1,layout=slider_layout),\n",
    ")\n",
    "climate_plot_adv.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What is the best configuration you could find?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    <ul>\n",
    "        <li> w_i1: 2.9 </li> \n",
    "        <li> w_o1: 1.9 </li> \n",
    "        <li> b1: -82 </li> \n",
    "        <li> w_i2: 4.4</li> \n",
    "        <li> w_o2: 1.3 </li>\n",
    "        <li> b2: -151 </li> \n",
    "        <li> Loss: 2.45 </li>       \n",
    "        <li> Wie haben erstmal 0C für alle Temperaturen > 28.2 Grad, danach geht es linear hoch mit der Steigung des ersten Neurons wi1 und bei kinickt es wieder bei ungefähr 34.3C mit der Steugung des zweiten Neurons wi2 </li>       \n",
    "    </ul>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Conclusion\n",
    "Using two ReLU neurons with individual biases better approximates a quadratic relationship than a single ReLU neuron, as it introduces two bends in the function. However, as network complexity grows, optimizing weights and biases becomes significantly harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Backpropagation\n",
    "\n",
    "The examples above relied on manual parameter adjustments. Real-world neural networks can have thousands or even millions of parameters, making manual tuning impossible. The solution is **backpropagation**, an algorithm that automatically:\n",
    "1. Performs **forward propagation**: passes inputs through the network to compute predictions.\n",
    "2. Calculates the **loss** by comparing predictions to target values.\n",
    "3. Uses the **chain rule** to compute **gradients** (partial derivatives of the loss) w.r.t. each weight and bias.\n",
    "4. **Updates** each parameter in the direction that **reduces** the loss, typically through **gradient descent**.\n",
    "\n",
    "### 1.6.1 Gradient Descent\n",
    "\n",
    "A common gradient-based update rule, where at each pass the parameters get updated based on loss feedback, is:\n",
    "$$\n",
    "  \\theta_{\\text{new}} \\;=\\; \\theta_{\\text{old}} \\;-\\; \\eta \\cdot \\frac{\\partial J}{\\partial \\theta},\n",
    "  \\quad (5)\n",
    "$$\n",
    "where:\n",
    "- $\\theta$ represents a parameter (e.g., weight or bias),\n",
    "- $\\eta$ is the **learning rate**,\n",
    "- $\\partial J / \\partial \\theta$ is the partial derivative of the loss w.r.t. $\\theta$.\n",
    "\n",
    "- If $\\eta$ is too large, updates might overshoot and the loss may explode or not converge.\n",
    "- If $\\eta$ is too small, training converges very slowly.\n",
    "\n",
    "**Epoch**: One full pass over the training data. Since we must recalculate gradients after each step, deep learning typically uses efficient frameworks (like PyTorch, TensorFlow, etc.) to handle these calculations automatically.\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/backprop.png\" alt=\"Visualization of gradient-based optimization on a loss surface\" width=\"600\"/>\n",
    "</center>\n",
    "<p style=\"text-align: center;\"><em>Figure 7: Gradient-based descent on a 3D loss surface.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_backprop = DualPlot(points_linreg, ranges_3d, ranges_linreg)\n",
    "trace_to_plot = go.Scatter3d(x=[], y=[], z=[], hoverinfo=\"none\", mode=\"lines\", line=dict(width=10, color=\"grey\"))\n",
    "\n",
    "plot_backprop.plot_3d.data.append(trace_to_plot)  # Expand 3D Plot to also plot traces\n",
    "plot_backprop.plot_3d.plot = go.FigureWidget(plot_backprop.plot_3d.data, plot_backprop.plot_3d.layout)\n",
    "plot_backprop.plot_3d.draw_time = 0\n",
    "\n",
    "\n",
    "def redraw_with_traces(plot_to_update: Interactive2DPlot, neuron: SimpleNeuron, trace_list: Dict[str, List[float]], points: Dict[str, List[float]]):  # executed every update step\n",
    "    plot_to_update.plot_3d.plot.data[2].x = trace_list[\"x\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].y = trace_list[\"y\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].z = trace_list[\"z\"]\n",
    "    plot_to_update.plot_3d.plot.data[1].x = [neuron.weight]\n",
    "    plot_to_update.plot_3d.plot.data[1].y = [neuron.bias]\n",
    "    plot_to_update.plot_3d.plot.data[1].z = [log_mse(neuron, points)]\n",
    "    plot_to_update.update()\n",
    "\n",
    "\n",
    "def add_traces(neuron: SimpleNeuron, points: Dict[str, List[float]], trace_list: Dict[str, List[float]]):  # executed every epoch\n",
    "    trace_list[\"x\"].extend([neuron.weight])\n",
    "    trace_list[\"y\"].extend([neuron.bias])\n",
    "    trace_list[\"z\"].extend([log_mse(neuron, points)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Implementing Backpropagation for a Single Neuron\n",
    "\n",
    "We return to a simpler scenario (a single neuron with no activation) to illustrate the idea:\n",
    "1. We compute the **forward pass** ($\\hat{y} = w \\cdot x + b$).\n",
    "2. We measure the **loss** using MSE.\n",
    "3. We compute the **partial derivatives** (gradients) w.r.t. $w$ and $b$.\n",
    "4. We **update** $w$ and $b$ with gradient descent.\n",
    "The final code will:\n",
    "- Plot the neuron’s movement in the 3D **loss surface** (weight vs. bias vs. log of MSE).\n",
    "- Show how the neuron’s line changes in 2D over the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Determine the Gradient <b>analytically!!</b>\n",
    "<ul>\n",
    "<li> <b>Finish the function below by yourself.</b>\n",
    "<li> There are multiple solutions to this, your algorithm may adjust the weight and bias in the right direction despite the gradient calculation being wrong.\n",
    "<li> <b>Benchmark:</b> If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.03, your solution is correct\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b>\n",
    "\n",
    "- If you are having trouble figuring the gradient out, try calculating the gradient by hand to grasp the core idea/algorithm behind the update steps.\n",
    "- Ask yourself: What are the components of the Loss-function? How does the Loss-function depend on the weight and bias variables, by which you have to differentiate?\n",
    "- If you aren't satisfied with the explanation, you could look at [resources](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) such as the one linked here, or other ones. Thankfully, there is a plethora of explanations in different languages available online.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron_loss_gradient(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> Dict[str, float]:\n",
    "\n",
    "    gradient_sum = dict(weight=0, bias=0) # contains the sum of the weight and bias gradient\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # for each point\n",
    "            # Hint: point_x and point_y are the current point values\n",
    "        '''\n",
    "        *) point_x ist der Eingabewert und point_y ist der richtige Ausgabewert.\n",
    "        *) Wir rechnen die Vorhersage für jeden Punkt aus.\n",
    "        *) Jetzt berechnen wir die Ableitung nach weight für diesen einen Punkt aus.\n",
    "        *) Jetzt berechnen wir die Ableitung nach bias für denselben Punkt aus.\n",
    "        *) Wir rechnen die Ableitung von der Loss-Funktion L = ( Der richtige Wert - Die Vorhersage )^2\n",
    "        *) Und die Vorhersage eines Neurons ist ja = weight * Eingabe + bias -> und da wir hier einen linearen Zusammenhang haben können wir direkt den Gradienten bilden.\n",
    "        *) Was wir im Ideal Fall erreichen wollen ist, dass der Gradient = 0 (Vektor) ist, dann heißt es dass wir keinen Fehler haben -> dafür summieren wir die Ableitung nach weght und bias um ein Gefühl dafür zu bekommen in welcher Richtung wir uns bewegen müssen, um nahe 0 zu kommen. \n",
    "        *) \n",
    "        '''\n",
    "        ŷ = neuron.weight * point_x + neuron.bias # Das ist die Vorhersage\n",
    "\n",
    "        gradient_sum[\"weight\"] += ( # sum up the gradient for each point\n",
    "\n",
    "            ### STUDENT CODE HERE (2 pts)\n",
    "            -2 * (point_y - ŷ) * point_x\n",
    "            ### STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "        gradient_sum[\"bias\"] += (\n",
    "            ### STUDENT CODE HERE (2 pts)\n",
    "            -2 * (point_y - ŷ)\n",
    "            ### STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "    gradient = dict(weight=gradient_sum[\"weight\"] / len(points[\"x\"]), bias=gradient_sum[\"bias\"] / len(points[\"x\"]))\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Adjust the Neuron\n",
    "<ul>\n",
    "\n",
    "<li> After finding the gradient you have to adjust the weight and bias of the neuron, based on the partial derivatives and the learning rate. You have to verify your results by training the neural network in an upcoming code block.\n",
    "<li> <b>Finish the function below by yourself.</b>\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b>\n",
    "<ul>\n",
    "    <li> This is an iterative function used on each neuron once per epoch.\n",
    "    <li> Use the neurons current weight and bias as a starting point and adjust it to improve the NN, as per equation (5).\n",
    "    <li> The entered learning rate scales the magnitude of the adjustment.\n",
    "    <li> Think about the direction of the loss gradient and the direction you want your loss to shift in.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_neuron(neuron: SimpleNeuron, gradient: Dict[str, float], learning_rate: float):\n",
    "    ### STUDENT CODE HERE (2 pts)\n",
    "    '''\n",
    "    *) Wir haben die weight und bias Werte im Block vorher bestimmt, jetzt wollen wir diese anpassen. \n",
    "    *) Das Anpassen heißt, dass wir die Werte gezielt so ändern, dass wir uns an dem globalen Minimum annähren, dies funktioniert mit der folgenden Regel.\n",
    "    *) unter gradient[ ] haben wir einen Dic, in dem der Durchschnitt aller wieghts/ biases gespeichert sind - Z.B. gradient = {\"weight\": -2.5, \"bias\": 1.8} \n",
    "    '''\n",
    "    neuron.weight = neuron.weight - learning_rate * gradient['weight']\n",
    "    neuron.bias = neuron.bias - learning_rate * gradient['bias']\n",
    "    ### STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Training Loop and Hyperparameters\n",
    "\n",
    "Once backpropagation is implemented, we define a training loop that:\n",
    "- Iterates for a chosen number of epochs.\n",
    "- In each epoch:\n",
    "  1. Calculates gradients via backpropagation.\n",
    "  2. Updates weights and biases.\n",
    "  3. (Optionally) logs or plots intermediate results.\n",
    "\n",
    "**Hyperparameters** like **learning rate** (`learning_rate`) and **epochs** are crucial. \n",
    "\n",
    "- A **large** learning rate may lead to divergence (loss grows uncontrollably).\n",
    "- A **small** learning rate can make convergence very slow.\n",
    "\n",
    "Once you find a good balance, training converges to a local minimum for this single-neuron problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def train(neuron: SimpleNeuron, points: Dict[str, List[float]], epochs: int, learning_rate: float, redraw_step: int, trace_list: Dict[str, List[float]]):\n",
    "    redraw_with_traces(neuron.plot, neuron, trace_list, points)\n",
    "    for i in range(1, epochs + 1):  # first Epoch is Epoch no.1\n",
    "        add_traces(neuron, points, trace_list)\n",
    "        gradient = simple_neuron_loss_gradient(neuron, points)\n",
    "        adjust_neuron(neuron, gradient, learning_rate)\n",
    "\n",
    "        if i % redraw_step == 0:\n",
    "            print(\"Epoch:{} \\t\".format(i), end=\"\")\n",
    "            redraw_with_traces(neuron.plot, neuron_backprop, trace_list, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Choose Hyperparameters and Train\n",
    "<ul>\n",
    "\n",
    "<li> Choose an optimal learning rate and number of epochs by trying out values and running the two cells below</li>\n",
    "<li> The default values required to verify your previous implementations are a learning rate of 0.03, 100 epochs with a redraw_step of 10.\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7a7b4fc0384c13b153883c07fb5b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.03 #keep this for benchmarking, change to play around\n",
    "epochs = 100 # keep this for benchmarking, change to play around\n",
    "redraw_step = 10 # update plot every n'th epoch. too slow? set this to a higher value (e.g. 100)\n",
    "\n",
    "# these values are taken as parameters by the train function below\n",
    "\n",
    "neuron_backprop = SimpleNeuron(plot_backprop)\n",
    "HBox((plot_backprop.plot_3d.plot, plot_backprop.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18.45\n",
      "Loss: 18.45\n",
      "Epoch:10 \tLoss: 0.56\n",
      "Epoch:20 \tLoss: 0.49\n",
      "Epoch:30 \tLoss: 0.44\n",
      "Epoch:40 \tLoss: 0.39\n",
      "Epoch:50 \tLoss: 0.35\n",
      "Epoch:60 \tLoss: 0.32\n",
      "Epoch:70 \tLoss: 0.29\n",
      "Epoch:80 \tLoss: 0.26\n",
      "Epoch:90 \tLoss: 0.24\n",
      "Epoch:100 \tLoss: 0.22\n"
     ]
    }
   ],
   "source": [
    "#run this cell to test algorithm\n",
    "np.random.seed(4) # keep this for benchmarking, remove to play around\n",
    "\n",
    "neuron_backprop.set_values(  # set weight and bias randomly\n",
    "    (5 * np.random.random() - 2.5), (5 * np.random.random() - 2.5)\n",
    ")\n",
    "trace_list1 = dict(x=[], y=[], z=[])\n",
    "\n",
    "train(neuron_backprop, points_linreg, epochs, learning_rate, redraw_step, trace_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.03, your solution is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only answer this after your algorithm has hit the benchmark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (4 pts):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to: <br>\n",
    "    \n",
    "1. What happens when you set the learning rate to 0.18? Explain this behavior. <br>\n",
    "2. What happens when you set the learning rate to 0.182? Explain this behavior. <br> \n",
    "3. What is the best learning rate you could find? (In terms of: lowest loss after 100 Epochs with lr=0.03) (Anything better than the benchmark loss of 0.22 is correct) <br>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. Die Lernrate hier ist zu groß und daher springen wir sozusagen über das Optimum und es wird nie erreicht, weil wir halt zu große Schritte beim korrigieren der Werte machen <br>\n",
    "2. Hier sind die Schritte NOCH größer, was dafür sorgt, dass der Training komplett instabil wird... Das geht sogar ganz in die Falsche Richtung ( LOSS wird GRÖßER!!!! ) <br>\n",
    "3. Mit einer Learning-Rate von 0.05 erreichen wir einen LOSS von 0.14 nach 100 Epochen ----- Mit einer Learning-Rate von 0.04 erreichen wir einen LOSS von 0.17 nach 100 Epochen <br> \n",
    "ZUSATZ FÜR MICH -> Wenn die Lernrate zu klein ist, dauert es EWIG bis wir zum Optimum kommen und wenn sie zu Groß ist schwingen wir hin und her oder geraten völlig wo anders! Deswegen muss man immer einen Mittelwert finden, bei dem sie weder zu Groß noch zu Klein ist <br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Machine/Deep Learning Notation\n",
    "\n",
    "- **Batch size**: The number of samples processed before updating parameters. Training may use:\n",
    "  - **Stochastic Gradient Descent (SGD)**: Update after each individual sample.\n",
    "  - **Mini-batch Gradient Descent**: Update after processing small batches (e.g., 32 samples).\n",
    "  - **Full-batch Gradient Descent**: Update after processing the entire dataset once.\n",
    "- **Epoch**: One full pass through the training data.\n",
    "- **Regularization** (e.g., L1, L2, Dropout) helps avoid overfitting by penalizing large weights or temporarily “dropping” neurons.\n",
    "- **Exploding/Vanishing Gradients** can occur in deep networks when gradients multiply across many layers. This is why ReLU is often used instead of sigmoid or tanh in deeper architectures. Modern deep networks may also include **skip (residual) connections** or other architectures to mitigate these issues.\n",
    "\n",
    "***\n",
    "**Further Reading**:  \n",
    "1. [Hornik, K. (1991).](https://www.sciencedirect.com/science/article/pii/089360809190009T?via%3Dihub) *Approximation capabilities of multilayer feedforward networks*. **Neural Networks**, 4(2), 251–257. \n",
    "\n",
    "  Demonstrates that feedforward networks with a single hidden layer and non-linear activations can approximate *any* continuous function on a compact set, given sufficiently many hidden neurons.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama25-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "479.492px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
