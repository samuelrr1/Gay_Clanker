{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Group Number:</b> \n",
    "<br><b>Name Group Member 1:</b> \n",
    "<br><b>u-Kürzel Group Member 1:</b> \n",
    "<br><b>Name Group Member 2:</b> \n",
    "<br><b>u-Kürzel Group Member 2:</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Decision trees are the building blocks of some of the most powerful **supervised learning** e.g. having a pre-defined target variable methods that are used today. if you have ever had to diagnose a problem with an appliance, car or a computer, there is a good chance you have encountered a troubleshooting flowchart. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal. \n",
    "<img src=\"images/snipped.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees (CART)\n",
    "\n",
    "Classification and Regression Trees is an acronym introduced by Leo Breiman in 1984 to refer to Decision Tree algorithms that can be used for predictive modeling problems. We will focus on the CART algorithm in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "The representation of the CART model is a binary decision tree. This is the same binary tree from algorithms and data structures (each node can have zero, one or two child nodes).\n",
    "\n",
    "A node represents a single input variable (X) and a split point on that variable, assuming the variable is numeric. The leaf or terminal nodes of the tree contain an output variable (Y) which is used to make a prediction. \n",
    "\n",
    "Creating a binary decision tree is actually a process of dividing up the input space. The so-called recursive binary splitting is used to separate the input space (greedy approach). This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function.\n",
    "\n",
    "The split with the best cost (lowest cost because we minimize costs) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function.\n",
    "\n",
    "- **Regression:** The cost function that is minimized to choose split points is the **sum squared error** across all training samples that fall within the rectangle.\n",
    "\n",
    "- **Classification:** The *Gini* cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.\n",
    "\n",
    "Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached.\n",
    "\n",
    "In this exercise we are only focusing on the classification property of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "The Gini index is the name of the cost function used to evaluate the splits in the dataset. A split involves one input attribute and one value for that attribute. A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst split e.g. 50/50 split results in a Gini score of 0.5 for a two class problem.\n",
    "\n",
    "Calculating the gini score is best demonstrated with an example:\n",
    "\n",
    "<img src=\"images/iris_tree.png\">\n",
    "\n",
    "Suppose you find an iris flower and you want to classify it. In the figure above, we start at the *root node*: this node asks wether the flower's pedal length is smaller than 2.45 cm. If it is the case, then we move down to the root's left child node. In this case it is a *leaf node*, because it does not have any children. \n",
    "\n",
    "Now suppose we find another flower with the difference that the petal length is greater than 2.45 cm. We move down to the root's right child node which is not a leaf node. It asks another question: is the petal width smaller than 1.75 cm ? If it is, then our flower is most likely an Iris-Versicolor. If not, it is probably an Iris-Virginica.\n",
    "\n",
    "For example, 100 trainig instances have a petal length greater than 2.45 cm, among which 54 have a petal width smaller than 1.75 cm. A node's value attribute tells us how many training instances of each class this node applies to: the bottom-right node applies to 0 Iris-Setosa, 1 Iris-Versicolor, and 45 Iris-Virginica. According to equation (\\ref{eq1}) the gini score is comupted as followed: $1-(0/54)^2-(49/54)^2-(5/54)^2 = 0.168$.\n",
    "\n",
    "\\begin{equation*}\n",
    "G_i = 1 - \\sum_{k=1}^n p_{i,k}^2\n",
    "\\end{equation*}\n",
    "where $p_{i,k}$ describes the ratio of class k instances among the training instances in the $i^{th}$ node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import *\n",
    "\n",
    "from lama.test_functions import SupervisedLearning_Tests\n",
    "from lama.helper import lama_del_wrap\n",
    "\n",
    "test_func = SupervisedLearning_Tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Decision Trees with scikit-learn\n",
    "\n",
    "Now we finally prepared our dataset to be used in Machine Learning algorithms. In the beginning, we outlined the idea of classification and regression with Decision Trees. In the following we will use the scikit-learn implementation of decision trees to perform the binary classification of the Titanic dataset.\n",
    "\n",
    "## 2.1 Decision tree without parameter tuning\n",
    "\n",
    "### 2.1.1 Importing the libraries\n",
    "The first step is to use the algorithm \"as is\" without tuning of any parameters. Therefore import the necessary library/function of scikit-learn that contains the DecisionTreeClassifier. To evaluate the model we need to generate a train and validation set using the train-test split of scikit-learn. Hence, import the necessary functions. As a baseline for comparing the performance of the trained decision tree, we need to import the DummyClassifier. The DummyClassifer https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html is able to classify data points uniformly at random or to always predict the most frequent label in the training set. Import the accuracy score of the package 'metrics' for the evaluation of the classification results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Use the following code for all needed imports\n",
    "\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Generating the Train, Validation and Test set\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Load the solution datasets that were prepared by the solution data preparation file (train_prepared.csv,...).\n",
    "<li> Get the column with the label data in both the train and test set and drop it from the datasets.\n",
    "<li>Split the 'train' part of the dataset in 80% training data and 20% validation data. Use parameter random_state = 17 for results reproducibility.\n",
    "<li> Note: You might need the original train set ('train' part before splitting it up into tr/val) later for cross-validation.\n",
    "</ul>\n",
    "    \n",
    "<b>Critical Information:</b> Using supervised learning datasets always consist of Labels and Features. Once you've trained your model, you will give it sets of new input containing features (Age, Sex etc.); it will return the predicted label (Survived) for that person.\\n\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train: pd.DataFrame\n",
    "x_val: pd.DataFrame\n",
    "x_test: pd.DataFrame\n",
    "y_train: pd.Series\n",
    "y_val: pd.Series\n",
    "y_test: pd.Series\n",
    "\n",
    "X_train_unsplitted: pd.DataFrame # train dataset before splitting it up into tr/val\n",
    "Y_train_unsplitted: pd.DataFrame\n",
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "#Sanity checks\n",
    "assert len(x_train.columns) & len(x_val.columns) == 8, f'Number of features expected was 8, but found {len(x_train.columns)} and {len(x_val.columns)} features.'\n",
    "assert len(x_train) & len(y_train) == 712, f'Number of samples expected in the train datasets was 712, but found {len(x_train)} and {len(y_train)} samples.'\n",
    "assert len(x_val) & len(y_val) == 179, f'Number of samples expected in the validation datasets was 179, but found {len(x_val)} and {len(x_val)} samples.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Validation of the Decision Tree\n",
    "\n",
    "To get an idea whether the classification with the model makes sense, we use the DummyClassifier that decides randomly. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Train the classifier with the corresponding parameter value 'most_frequent' for strategy\n",
    "<li> Use for the parameter random_state = 17 (for results reproducibility)\n",
    "<li> Calculate and display the accuray to get the classification accuracy on the validation data (Hint: Look at `sklearn.metrics.accuracy_score`)\n",
    "<li> Hint: Check the website from scikit-learn to import the Classifier, train it, predict with it and calculate the accuracy\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Train a decision tree (DecisionTreeClassifier) with a maximum depth of 2\n",
    "<li> Evaluate and display the accuracy metric on the validation data.\n",
    "<li> Use parameter random_state = 17 for results reproducibility.\n",
    "<li> Hint: Syntax or functions with this classifier are the same for training etc.\n",
    "    </li>\n",
    "    \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# Sanity check - compare your results\n",
    "test_func.test_decision_tree_acc(accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to:<br>\n",
    "\n",
    "1. What is your interpretation of the DummyClassifiers classification accuracy? <br>\n",
    "2. What do you observe if we compare the classification accuracy of the DecisionTree to that of the DummyClassifier?\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "    \n",
    "1. <br>\n",
    "2. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Understanding the trained model \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Plot the tree with sklearn.tree.plot_tree\n",
    "<li> Print the feature names as well as the class names according to the dataset.\n",
    "<li> Hint: Use dataframe.columns.values\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b>  Which features are used to make predictions in the created decision tree? Which of the remaining splits (in the last row of the tree) is currently the most accurate?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Testing generalization\n",
    "\n",
    "In the previous tasks we have evaluated the performance of our algorithm on a single train-test split of our train dataset. Let's use cross-validation to get a better estimate of the generalization error. \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Import the necessary library for cross validation with StratifiedKFold.\n",
    "\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Perform a 5-fold stratified cross validation\n",
    "<li> Calculate the mean accuracy and the standard deviation of the accuracy\n",
    "<li> Use a maximum depth of 2 and random_state = 17 for tree. Do not use random_state for the folds.\n",
    "<li> Do not forget to use the whole training set (before splitting it up into train,val)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (3pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# Sanity check - compare your results\n",
    "test_func.test_cross_validation_acc(acc1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parameter Optimization for Decision Trees\n",
    "\n",
    "The most important parameter of a decision tree is the depth of the tree. Hence, it is necessary to evaluate different depths of the tree to achieve the optimal performance regarding the classification accuracy. For that purpose we use grid search combined with the cross validation process we have used before. Luckily, scikit-learn has already implemented a nice and easy to use interface for that problem. The function is called `GridSearchCV` and can be found in the sklearn.model_selection library. \n",
    "\n",
    "### 2.2.1 Using Grid Search Cross-Validation to optimize the tree depth\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Load the GridSearchCV library and train a decision tree (DecisionTreeClassifier, random_state = 17)\n",
    "<li> Find the optimal maximum depth using 5-fold stratified cross-validation (without RandomState)\n",
    "<li> Vary the depth of the tree between 1 and 13. range(1,13)\n",
    "<li> Do not forget to use the whole training set (before splitting it up into train,val)\n",
    "<li> Hint: Use the scikit-learn website for more information on the functions\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Draw a plot to show the mean accuracy over depth \n",
    "<li> Use the attribute <code>.cv_results</code> to get mean accuracy by using 'mean_test_score'\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (4pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Use the code block below to find out and display the following: \n",
    "    <ul>\n",
    "    <li> What are the best parameter values? </li>\n",
    "    <li> What is the accuracy (cross-val) of the model with that tree depth?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# Sanity check - compare your results\n",
    "test_func.test_best_param(best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the optimal parameter regarding our training data. Finally, we can evaluate the performance using all of our training data to train and test with the test data set. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "<ul>\n",
    "<li> Train a decision tree with the maximum depth you got above using all training data (no cross-validation)\n",
    "<li> Compute the accuracy on the test data set. Use parameter random_state = 17 for reproducibility.\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree: DecisionTreeClassifier\n",
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "tree.plot_tree(decision_tree, \n",
    "               feature_names=x_train.columns.values,\n",
    "               class_names=['Dead','Survived'], \n",
    "               rounded=True, \n",
    "               fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Comparing the results to the un-optimized version\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Сalculate the effect of GridSearchCV\n",
    "<ul>\n",
    "<li> Use the expression (acc2 - acc1) / acc1 * 100%\n",
    "<li> acc1 and acc2 are accuracies of cross-validation before and after tuning max_depth with GridSearchCV\n",
    "<li> Hint: acc1 was already used before optimization, just calculate acc2 for comparison\n",
    "<li> Print the improvement (calculated by the expression) and the mean accuracy of the optimized dec_tree\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# Sanity check - compare your results\n",
    "test_func.test_GridSearchCV_acc(acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What are the advantages of the grid search process?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Influence of Scaling\n",
    "\n",
    "As a last step we want to evaluate the influence of different scaling to our training data.\n",
    "\n",
    "### 2.3.1 Scaling the Datasets with Standard Scaler and MinMaxScaler\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> \n",
    "\n",
    "Load the functions needed for the StandardScaler and the MinMaxScaler included in sklearn.preprocessing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE (1pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Prepare two different datasets, one scaled with StandardScaler and the other one using MinMaxScaler.\n",
    "<ul>\n",
    "<li> Create the respective scalers and use the <code>.fit_transform()</code> method by using the whole training dataset\n",
    "<li> Then transform the test dataset with the 'fitted' scalers using the transform function\n",
    "\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std: np.ndarray\n",
    "x_test_std: np.ndarray\n",
    "x_train_minMax: np.ndarray\n",
    "x_test_minMax: np.ndarray\n",
    "\n",
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE   \n",
    "\n",
    "#Sanity Check\n",
    "assert (np.abs(np.mean(x_train_std)) < 0.000000001) ^ (np.abs(np.mean(x_test_std)) < 0.000000001), f'Expected either train or test set to be fitted for scaling.'\n",
    "assert (np.std(x_train_std) - 1 < 0.000000001) ^ (np.std(x_test_std) - 1 < 0.000000001), f'Expected either train or test set to be fitted for scaling.'\n",
    "assert (np.max(x_train_minMax[:,2]) == 1) ^ (np.max(x_test_minMax[:,2]) == 1), f'Expected either train or test set to be fitted for scaling.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Evaluate the performance on the scaled datasets\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Now train two decision tree models for each of the newly scaled datasets (with random_state = 17)\n",
    "<ul>\n",
    "<li> Calculate the accuracy on the test dataset for both datasets</li>\n",
    "<li> Use a maximum depth of 3 for the training process </li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler dataset\n",
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaler dataset\n",
    "# STUDENT CODE HERE (2pt)\n",
    "\n",
    "# STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> Compare the accuracy of the result of both scaling options to the original (without scaled datasets). What do you observe and what do you think is the explaination? Do you suggest to apply scaling in general?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Your Answer:</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 RandomForests with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use not only one DecisionTree but several. By doing so we take multiple classifiers outputs into account than only a single one (Ensemble method - in this case Bagging). RandomForest classifiers are less prone to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=17)\n",
    "fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "scores = cross_val_score(random_forest, X_train_unsplitted, Y_train_unsplitted, cv=fold)\n",
    "print(f'Accuracy: {scores.mean():0.2f} (+/- {scores.std() * 2:0.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly as good as our optimized decision tree and better than our unoptimized version by only using the RandomForestClassifiers default values and not being optimized. You should carry this MachineLearningTool in your pocket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
