{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<b>Group Number:</b> \n", "<br><b>Name Group Member 1:</b> \n", "<br><b>u-K\u00fcrzel Group Member 1:</b> \n", "<br><b>Name Group Member 2:</b> \n", "<br><b>u-K\u00fcrzel Group Member 2:</b> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3 K-Nearest Neighbors (KNN)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What is KNN ?\n", "\n", "Let\u2019s first start by establishing some definitions and notations. We will use $x$ to denote a *feature* (aka. predictor, attribute) and $y$ to denote the *target* (aka. label, class) we are trying to predict.\n", "\n", "KNN falls in the **supervised learning** family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations $(x,y)$ and would like to capture the relationship between $x$ and $y$. More formally, our goal is to learn a function $h:X\u2192Y$ so that given an unseen observation $x, h(x)$ can confidently predict the corresponding output $y$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction\n", "\n", "In the classification setting, the K-nearest neighbor algorithm essentially boils down to forming a majority vote between the K most similar instances to a given \u201cunseen\u201d observation. Similarity is defined according to a distance metric between two data points. Instances given as $x = <a_1(x),a_2(x),...,a_n(x)>$ are represented as points in the n-dimensional space $\\mathbb{R}^n$. Their relationships/distances can be formulated as the euclidean distance:\n", "\n", "\\begin{equation}\n", "d(x_i,x_j) \\equiv \\sqrt{\\sum_{r=1}^{n} (a_r(x_i) - a_r(x_j))^2}\n", "\\end{equation}\n", "\n", "In this case the distance metric is chosen to be the Euclidean distance, that is a popular choice. Other distance metrics may be more suitable for a given setting, including the Manhatten, Chebyshev and Hamming distance. \n", "\n", "A function $h$ is then learned from $\\mathbb{R}^n$ $\\rightarrow$ $V$, $V$ being a finite set of all possible classes. \n", "\n", "### Algorithm\n", "\n", "For every given instance $x_i$ add it to the list of training_examples. \n", "\n", "**Inference:**\n", "\n", "A new instance $x_q$ needs to be classified and $x_1,...,x_k$ are the $k$ closest instances to $x_q$ (known after computing all distances between training examples and the new instance). Note that $K$ is usually odd to prevent tie situations.\n", "Then the new classification of $x_q$ is given by:\n", "\n", "\\begin{equation}\n", "h(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k}\\delta (v,y_i)\n", "\\end{equation}\n", "\n", "with \n", "\\begin{equation}\n", "\\delta (a,b) = \\begin{cases}\n", "1, \\text{if } a=b \\\\\n", "0, else\n", "\\end{cases}\n", "\\end{equation}\n", " \n", "### Example: \n", "\n", "Classification with $K=5$:\n", "\n", "<img src=\"images/knn_5.png\" width=\"800\">\n", "\n", "__$\\rightarrow$The new instance is classified as a square.__\n", "\n", "Classification with $K=1$:\n", "<img src=\"images/knn_1.png\" width=\"800\">\n", "\n", "__$\\rightarrow$The new instance is classified as a triangle.__ \n", "\n", "\n", "\n", "\n", "If $k = 1$ the decision boundaries will end up being the same as in a [Voronoi](https://en.wikipedia.org/wiki/Voronoi_diagram) diagram. Whereas using circles the perspective is to find the next K nearest points to the new instance, a Voronoi shows regions, in which new instances are assigned to the known data points. (The perspective is now on the squares and triangles that will assign new points next to them to their class.)\n", "\n", "**Additional Information:**\n", "\n", "- In general it is often useful to normalize the input vectors so that input dimensions are not skewed so much.\n", "\n", "- There is the possibility to use distance based weights for instances rather than uniformly taking known instances into consideration. Therefore the equation given above changes to:\n", "\n", "    \\begin{equation}\n", "    f(x_q) \\leftarrow arg\\max_{v \\in V}\\sum_{i=1}^{k} w_i \\delta (v,c(x_i))\n", "    \\end{equation}\n", "\n", "    and the weights are given by:\n", "\n", "    \\begin{equation}\n", "     w_i \\equiv \\frac{1}{d(x_q,x_i)^2}\n", "    \\end{equation}\n", "    \n", "- __Cover and Hart 1967__: As $n \\rightarrow \\infty$, the 1-NN error is no more than twice the error of the Bayes Optimal classifier.(Similar guarantees hold for k>1.) [Source](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n", "\n", "- __Curse of dimensionality__: The inductive bias the K-NN algorithm makes is, that similar points share labels. In high dimensional spaces this assumption holds not as good as lower dimensional spaces. This is because, points are not close to each other if drawn uniformly in every dimension. See also: [Source](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)\n", "\n", "- Reduce high dimensional data by PCA or SVD, intrinsic dimensions of the data should be lower dimensional or do not make use of all dimensions the data is given in\n", "\n", "- K-NN algorithm becomes slow as n or the dimensions d increase, not feasible inference time\n", "\n", "- K-NN algorithm becomes more accurate as n increases"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Imports"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "from typing import *\n", "\n", "from lama.test_functions import SupervisedLearning_Tests\n", "from lama.helper import lama_del_wrap\n", "\n", "test_func = SupervisedLearning_Tests()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.1 Load the datasets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Task:</b> Load the prepared datasets training and test set, because we are only going to use GridSearchCV\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["x_train: pd.DataFrame\n", "x_test: pd.DataFrame\n", "y_train: pd.Series\n", "y_test: pd.Series\n", "\n", "# STUDENT CODE HERE (2pt)\n", "\n", "# STUDENT CODE until HERE\n", "\n", "# Sanity check\n", "assert len(x_train.columns) == 8, f'Number of features expected was 8, but found {len(x_train.columns)} features.'\n", "assert len(x_test.columns) == 8, f'Number of features expected was 8, but found {len(x_test.columns)} features.'\n", "assert len(x_train) == 891, f'Number of samples expected in the train datasets is 891, but found {len(x_train)} samples.'\n", "assert len(x_test) == 418, f'Number of samples expected in the test datasets is 418, but found {len(x_test)} samples.'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Task:</b> Normalize the train and test data with Min Max Scaling. You may not use a MinMaxScaler from scikit, since we have to ensure that the data type is still a Pandas dataframe, due to visualization later in this notebook. Watch out for variable names, when the cell is executed several times.\n", "</div>"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Normalize the data\n", "''' pay attention that you only use the train data to normalize your test data. \n", "    Otherwise the model passively sees information from the test set, because it influences the values we use to normalize the data.\n", "'''\n", "# STUDENT CODE HERE (1pt)\n", "\n", "# STUDENT CODE until HERE\n", "\n", "assert (x_train['Age'].max() == 1) ^ (x_test['Age'].max() == 1), f'Expected either train or test set to be fitted for scaling.'\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Imports the necessary modules\n", "\n", "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n", "from matplotlib.colors import ListedColormap\n", "num_neighbors = 1\n", "radius = 100"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\"The k-neighbors classification in KNeighborsClassifier is the most commonly used technique. The optimal choice of the value is highly data-dependent: in general a larger k suppresses the effects of noise, but makes the classification boundaries less distinct.\n", "\n", "In cases where the data is not uniformly sampled, radius-based neighbors classification in RadiusNeighborsClassifier can be a better choice. The user specifies a fixed radius , such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called \u201ccurse of dimensionality\u201d.\n", "\n", "The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\" [Source](https://scikit-learn.org/stable/modules/neighbors.html)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"scrolled": true}, "outputs": [], "source": ["neigh = KNeighborsClassifier(n_neighbors=num_neighbors)\n", "neigh.fit(x_train, y_train)\n", "\n", "neigh_r = RadiusNeighborsClassifier(radius)\n", "neigh_r.fit(x_train, y_train)\n", "\n", "print(f'Test score of K-Nearest Neighbor: {neigh.score(x_test, y_test)}')\n", "print(f'Test score of K-Nearest Neighbor with radius: {neigh_r.score(x_test, y_test)}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.2 Optimization"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2.1 Find the optimal number of neighbors for the Conventional Classifier (KNN)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Task:</b> \n", "<ul>\n", "<li> Optimize not only with respect to one, but two parameters\n", "<li> Optimize the k in nearest_n_arr as well as the weight distribution.\n", "<li> Use GridSearchCV and a dictionary for k_params (In the KNN task you do not need to use StratifiedKfold, just use the parameter cv, which does a stratisfied fold but without the random_state parameter.)\n", "\n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Nearest Neighbors values to optimize the model\n", "nearest_n_arr = range(1,20)  # natural numbers\n", "\n", "# Necessary for plotting\n", "length_a = len(nearest_n_arr)  \n", "\n", "k_params: Dict[str, List[Any]]\n", "k_model: GridSearchCV\n", "    \n", "# STUDENT CODE HERE (2pt)\n", "\n", "# STUDENT CODE until HERE\n", "\n", "\n", "## Plot your results\n", "\n", "neighourhood = k_params['n_neighbors']\n", "scores = k_model.cv_results_['mean_test_score']\n", "scores_std = k_model.cv_results_['std_test_score']\n", "\n", "# Resorting for plots\n", "scores = np.append(scores[0::2], scores[1::2])\n", "scores_std = np.append(scores_std[0::2], scores_std[1::2])\n", "\n", "neighourhood = k_params['n_neighbors']\n", "\n", "plt.figure().set_size_inches(20, 5)\n", "plt.xlabel('Number of Neighbors')\n", "plt.xlim(neighourhood[0],neighourhood[-1])\n", "plt.ylabel('Mean Test Score')\n", "plt.plot(neighourhood, scores[:length_a],'g--',\n", "        label='uniform')\n", "plt.plot(neighourhood, scores[length_a:]\n", "         , 'b--',label='distance')\n", "plt.title('Optimal Number of Neighbors w.r.t. weight distribution')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2.2 Get the best model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Task:</b> \n", "<ul>\n", "<li> Get the best parameters and the correspeonding score values.\n", "<li> Print them! \n", "</ul>\n", "</div>"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Get the params, and the test score of the best model, print the values\n", "\n", "# STUDENT CODE HERE (3pt)\n", "\n", "# STUDENT CODE until HERE\n", "\n", "# Sanity check - compare your results\n", "test_func.test_best_neighbors(best_neighbors)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.4 Visualization\n", "\n", "We will take the two features of the training set and look where the decision boundaries are drawn for a KNN trained on them."]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def plot_knn(n_neighbors: int, radius: float, f_names: List[str], X: np.ndarray, y: np.ndarray):\n", "    \n", "    # Create color maps\n", "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n", "    cmap_bold = ListedColormap(['#FF0000','#0000FF'])\n", "    \n", "    h = .02\n", "    for weights in ['uniform', 'distance']:\n", "        # we create an instance of Neighbours Classifier and fit the data.\n", "        clf = KNeighborsClassifier(n_neighbors, weights=weights)\n", "        clf_r = RadiusNeighborsClassifier(radius=radius, weights=weights, outlier_label=0)\n", "        clf.fit(X, y)\n", "        clf_r.fit(X,y)\n", "        \n", "        # Plot the decision boundary. For that, we will assign a color to each\n", "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n", "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n", "                             np.arange(y_min, y_max, h))\n", "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "        Z2 = clf_r.predict(np.c_[xx.ravel(), yy.ravel()])\n", "        \n", "        # Put the result into a color plot\n", "        Z = Z.reshape(xx.shape)\n", "        Z2 = Z2.reshape(xx.shape)\n", "         \n", "        f,ax = plt.subplots(1,2,figsize=(20,5))\n", "        \n", "        ax[0].pcolormesh(xx, yy, Z, cmap=cmap_light)\n", "        sc1 = ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n", "                    edgecolor='k', s=20)\n", "\n", "        ax[0].set_xlim(xx.min(), xx.max())\n", "        ax[0].set_ylim(yy.min(), yy.max())\n", "        ax[0].set_xlabel(f_names[0])\n", "        ax[0].set_ylabel(f_names[1])\n", "        ax[0].set_title(f'K-Nearest Neighbor with (k={n_neighbors}, weights = \\'{weights}\\')')\n", "        f.colorbar(sc1,ax = ax[0]) \n", "        \n", "        ax[1].pcolormesh(xx, yy, Z2, cmap=cmap_light)\n", "        sc2 = ax[1].scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n", "                    edgecolor='k', s=20)\n", "        \n", "        ax[1].set_xlim(xx.min(), xx.max())\n", "        ax[1].set_ylim(yy.min(), yy.max())\n", "        ax[1].set_xlabel(f_names[0])\n", "        ax[1].set_ylabel(f_names[1])\n", "        ax[1].set_title(f'Radius Neighbors Classifier with (R= {radius:0.3f}, weights = \\'{weights}\\')')\n", "        f.colorbar(sc1, ax = ax[1]) \n", "       \n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# Reconsider all features we have\n", "x_test.columns"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Choose features to compare\n", "features_compared = ['Pclass', 'Sex']\n", "\n", "# Plot them\n", "plot_knn(2, 0.3, features_compared, np.array(x_test[features_compared]), y_test.values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Question (1pt):</b> Which features were compared in the default setting?\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Your Answer:</b> \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Task:</b> Apply the same visualization to continous features if possible.\n", "</li>\n", "</ul>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Question (2pt):</b> Answer the following questions in the answer block below and indicate which question your answer is referring to:<br>\n", "\n", "1. Why are there in the default setting only 6 points?<br>\n", "2. Are previous determined optimal values for KNN and RNN (via GridSearchCV) reliable for these plots?<br>\n", "3. What happens to the decision boundaries as you increase k? You might change the features to see it.<br>\n", "4. Why is normalization generally recommended to improve the performance of the KNN algorithm? Think about the scale of the individual input features. (Leave out normalization right at the beginning of this Task)<br>\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Your Answer:</b> \n", "    \n", "1.  <br>\n", "2. <br>\n", "3. <br>\n", "4. \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.5 Visualization by different methods"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Below there are used some ways to visualize data in higher dimensions, since we were looking with the knn_plots only at two dimensions."]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.decomposition import PCA as sklearnPCA\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n", "from sklearn.datasets import make_blobs\n", "\n", "from pandas.plotting import parallel_coordinates"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.5.1 Dimensionality Reduction with PCA\n", "\n", "Can be used as well for preprocessing purposes (also with K-NNs), but is only used for visualization purposes in here."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["pca = sklearnPCA(n_components=2) #2-dimensional PCA\n", "transformed = pd.DataFrame(pca.fit_transform(x_train))"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,5))\n", "plt.scatter(transformed[y_train==0][0], transformed[y_train==0][1], label='Not Survived', c='red')\n", "plt.scatter(transformed[y_train==1][0], transformed[y_train==1][1], label='Survived', c='green')\n", "plt.title('PCA plot of the Titanic features')\n", "\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.5.2 Parallel Coordinates"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One line describes one sample in you data. By looking at 'continuous' values (more than just a handful of discrete values), you can see that long names tended to survive for example."]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["data_norm = pd.concat([x_train, y_train], axis=1)\n", "\n", "# Perform parallel coordinate plot\n", "plt.figure(figsize=(20,5))\n", "parallel_coordinates(data_norm, 'Survived')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Question (1pt):</b>  What happens if the data is not normalized in these methods? Check by yourself.\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Your Answer:</b> \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See also: http://www.apnorton.com/blog/2016/12/19/Visualizing-Multidimensional-Data-in-Python/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Further Reading\n", "\n", "\n", "- Bagging\n", "- Logistic Regression\n", "- Random Forests\n", "- Combination of Bagging and GridSearchCV"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Feedback and Recap\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Question (3pt):</b>  Please conclude in a few sentences what you learned in this exercise\n", "</div>\n", "\n", "<div class=\"alert alert-block alert-success\">\n", "<b>Your Answer:</b> \n", "</div>\n", "\n", "## And give us feedback if you like\n", "\n", "\n", "1) Do you think this task was designed well? \n", "\n", "2) Where can we improve this task?\n", "\n", "<strong>Thanks for participating in LAMA! :)</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.5"}, "metadata": {"interpreter": {"hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"}}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}, "vscode": {"interpreter": {"hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}}, "nbformat": 4, "nbformat_minor": 4}